#+title:     Final-Year Project
#+author:    Jeremiah Via
#+email:     jeremiah.via@gmail.com
#+options:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+options:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+infojs_opt: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+startup: hidestars indent

* Project
** DONE Find class where calculation takes place
org.bham.aucom.fts.tranform.CalcEntropyAvgScore

T2GramModelTrainer
** DONE Make a simple 3-component chain
** TODO Get data running on 3-chain
** TODO Get data with 3-chain taking connections into account
* Logbook

Weekly logs from the project.

** 2011-10-03

   - Dora experiments

** 2011-10-10

I investigated how I will publish message signatures from ROS to
aucom and my system. At first, I looked at using Java since writing
a networking client is something I know how to do. Unfortunately,
significant portions of rosjava are unimplemented (and without any
mention in the documentation). I next considered Python, since it
is probably the most tightly integrated language into ROS. There
are a lot of ways to access the Master node, but after reading more
documentation, I saw that the Master node simple helps other nodes
find each other. After the connection is made, all communication is
peer-to-peer. This seriously threw a wrench in my plans for
automatic publishing of all topics.

After more consideration, I decided to only listen to topics
provided by a configuration file or commands line argument. This is
probably the better approach, because a learned model can work on
only part of the system. There is still the tricky bit of taking
this list, importing all of the relevant msgs, and then registering
a separate callback function for each event type. There are a
number of ways to solve this: 1) Use Lisp macros to generate
callback functions at runtime. This could be pretty simple since
the callback only has to do something like

#+BEGIN_SRC lisp
     (publish timestamp topic-name topic-type)
#+END_SRC

which would be easy to generate as a macro. Other approaches are
not likely to be as nice as this one.

--------

I met with Peter Tino for advice about using a neural network to
classify system state.  He told me not to necessarily limit myself
to using a neural network, as there might be other approaches that
could work more easily. Some examples he mentioned were one-class
learning, support vector machines, and a graph kernel. I will have
to look into these approaches. He stressed that the most important
thing I have to figure out is how to represent the data. Temporal
data is complex and I must spend time figuring out a representation
from which I can learn.

** 2011-10-17

Met with Nick & Marc this week and got some brief feedback
regarding my project proposal. Marc said I should have been more
explicit in my time line, in that the events I listed on my Gantt
chart are not specific enough for someone who is not me, because
they won't know what these events mean. Ela commented that I did
not determine my criteria for success, which is an excellent point
I had not considered. Marc said that I should avoid selecting any
criteria that can be answered with a yes or no question. Rather, I
should select criteria which allows me to compare on multiple
dimensions.

** 2012-01-09
*** Wednesday

Started worked on ROS-CAST connection. I created all of the
necessary classes and now I need to fill in the method stubs.

*** Thursday

Met with Nick today. We talked about the project and which direction
to head. Some of the things we talked about:

- Heart beat
  - what can it be used for
  - limitations on training and testing
- What kind of variations in training can aucom take?
  - Flat curve, i.e., random events like faces cannot be predicted

An important point he mentioned was what kind of story I want to tell
with my project. I need to provide a deeper understanding and go
beyond what Raphael, i.e., don't just implement on other systems.

He suggested that I start from the beginning so I can find a goal for
aucom. I should determine what it can and cannot do and relate these
to real-life scenarios. Essentially, I need to find the story that
says "his system can't do X, this is how we extended it so it could do
X".

An interesting problem is determining how well the system scales.

** 2012-01-16

Quantitative:

- Determine memory & cpu
- compare to score
- experiments on the score

Qualitative:
- What changes by doing it this way?
- Is there anything I may lose, however subtle?

Is it even possible to model the numbers across new chains?


3-chain run with dynamic start, i.e., start t random times and see how
it effects the scoring over time.

Overall testing:
- Wireless connectivity (throttling network connectivity)
- Camera lead falls out
- Component lockup


coding:
- set non-conecteds ones to 0
- thrn try bigger change

** 2012-01-23
*** FYP Talk: Evaluating Software

- How well does the project meet the spec?

- How systematic has evaluation been?

- Have a section & break it up into quantitative analysis &
  qualitative analysis.

- Evaluating the process
  - the design decision made along the way
  - need to justify the steps taken

- Need to identify differences between other approaches, i.e., prior
  techniques.

- *Qualitative analysis*
  - How could the end user receive this?
  - Focus on feature analysis on finished project.
  - Case studies? Hard to replicate, but used to evaluate
    scalability. 

- *Literature review*
  - Appears in background but can also be used to in evaluation by
    comparing features.
  - Be ethical & honest.
  - There is always some delta by which to compare.

- *Feature analysis*
  - List all features & define them with a scale
  - Evaluate this software against features.
  - Document for evaluation on FYP website.

*** Observations

0.1 seems a good cutoff

The model is represented as a matrix of probability
distributions. What is a good way to model "empty" for a probability
distribution? I don't think there is one.

** 2012-01-30
*** Meeting with Nick

Key questions I need to answer:
- What can I show different between the two?
- Where and how will differences manifest?

I need to create systems that test the algorithm and ones that are
necessarily faithful to a real system. It's more important to find the
limits of the algorithm.

Main goal: results! I need to create graphs to compare
performance. This is of critical importance!

*** Talk with Raphael

Experiments worth conducting for Raphael:
- Component crash
- Resource starvation

Experiments to test the algorithm:
- Really long chains
- Many disjoint chains
- Tons of individual components

Score does not change because there is a bug in the system right now.

* Dissertation
:PROPERTIES:
:EXPORT_FILE_NAME: dissertation
:END:
** Abstract

Fault-detection in robotics systems is a difficult task and as systems
are becoming more larger and complex, subtle errors are becoming
harder to diagnose. Traditional fault-detection approaches have relied
on explicit modeling of component behavior, but this technique does
not scale to complex robots operating in dynamic environments. A new
technique which involves making the robot self-aware to the internal
state of its various components is examined. The aim of this project
is to implement and then measure the efficacy of this probabilistic
self-awareness model for the robotics middleware CAST, and if time
allows, deal with shortcomings of the original approach.

/Keywords/: robotics, fault detection, machine learning

** Acknowledgments
** Motivation
*** What is the problem & why is it important?
The number of robots and their tasks is increasing. More and more
robots are becoming part of the human daily experience.

 There are now robots which clean the house, assist in surgery, and
automate the construction of goods.

There are robots in factories automating difficult or repetitive
tasks. There are an increasing number of domestic robots being used
for assistance in the home or as entertainment. It is important for
these robots to function correctly, and if unable to do so, to degrade
gracefully to minimize harm to themselves and others. To do this,
robots need some way to determine their own operating conditions.
Detecting faults within robot systems is a hard problem.

The importance of equipping a robot with the ability of self-awareness
to its internal state increases as humans interact more with robots.
One could imagine a robot in the home which assisted an elderly person
or \textit{another situation}. A malfunction in these situations a
malfunction could cause serious harm to a human. One could also
imagine robotic arms at a factory building cars. A malfunction could
cause damage to products or the arm itself resulting in a loss of
factory output. These examples underscore the importance of detecting
and handling faults within a robot system.

- the problem is detecting faults within the software of complex
  robotics systems
- goal: giving a robot a sense of self-awareness can allow it to
  degrade gracefully
- faulty robots are dangerous
- a fault can cause injury to a person
- a fault can damage products or the robot
- results in loss of property
*** Who else has wanted to solve it and how did they do it?
- two main approaches: model-based & data-driven
- from raph's paper:
  5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22
- model-based
  - analytic:
    - "used in the design of control systems where the models are
      constructed based on fundamental assumptions"
    - time consuming, so not much use in the complex cognitive systems
      of modern robots
    - 6,7
  - knowledge-based:
    - "qualitative models of the system are used to
      detect and diagnose faults"
    - require expert knowledge to design the model
    - 5,8,9,10,11,12,13,14,15,16,17
- data-driven
  - 18,19,20,21,22,1
*** What was the goal of my project?
- modify aucom to work with CAST
- implement a CAST component to work with aucom
- run experiments to determine the efficacy of this approach
- extension was to modify the learned model to increase efficiency
*** Introduce the rest of the dissertation
**** What is the point of the literature review?
**** Why talk about the theory?
**** Why talk about the original system and then analyze it?
**** What is the point of the improvements?
**** Why talk about project management?
**** Why evaluate the project?
** Literature review
**** What kinds of approaches are there?
**** What are their main ideas?
**** Who has used the model-based approach and what did they do?
**** Who has used the data-driven approach and what did they do?
**** How does aucom fit in with these approaches?
** Theory
- Explain the math concisely.
- Give detailed explanation of the learned model so later asymptotic
  analysis fits the story.
- Mention CAST and give a brief overview of how it works, perhaps
  juxtaposing it to XCF (although this may be out of the
  dissertation scope).
** Original system:
*** Implementation
**** CAST
**** FTS graph
*** Experimental results
**** introduce the experiments for the rest of the report
***** record fault tracking time
**** 3x1 experiments
**** 4x4 experiment results
**** 10x1 experiment results
**** dora experiment
*** Asymptotic analysis
**** Show mathematical derivation of model memory
**** Show mathematical dervaition of score calculation
** Connection-based model
*** Idea
*** Implementation
*** Asymptotic analysis
**** model
**** score calculation
*** Experimental results
**** 3x1 experiments
**** 4x4 experiment results
**** 10x1 experiment results
**** dora experiment
** Metronome-based approach
*** Idea
*** Implementation
*** Asymptotic analysis
**** model
**** score calculation
*** Experimental results
**** 3x1 experiments
**** 4x4 experiment results
**** 10x1 experiment results
**** dora experiment
**** ROC analysis of the three approaches
** Project management
*** Managing tasks & deadlines
**** Github issues
*** Managing code
**** maven
**** Jenkins
**** git
** Project evaluation
*** What was good?
**** project planning w.r.t. summer work
*** What can be learned?
**** sticking with it when intial results are bad
**** setting better goals
** Conclusion
*** Conclude story
*** Future work
* COMMENT Appendix

#  LocalWords:  SRC Gantt Ela aucom
