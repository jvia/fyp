#+title:
#+author:  Jeremiah M. Via
#+options: H:4 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+startup: hidestars indent
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LATEX_CLASS: dissertation
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{program}
#+LATEX_HEADER: \NumberProgramstrue

#+begin_latex
\begin{titlepage}
%% Set the line spacing to 1 for the title page.
\begin{spacing}{1}
\begin{large}
\begin{center}
\mbox{}
\vfill
\begin{sc}
A Data-Driven Self-Awareness Model for Robotics Systems \\
\end{sc}
\vfill
Jeremiah M. Via \\
Supervisor: Nick Hawes \\
\vspace*{4mm}
\includegraphics[width=50mm]{crest.png}\\
Submitted in conformity with the requirements\\
for the degree of Artificial Intelligence \& Computer Science\\
School of Computer Science\\
University of Birmingham\\
\vfill
Copyright {\copyright} 2012 School of Computer Science, University of Birmingham\\
\vspace*{.2in}
\end{center}
\end{large}
\end{spacing}
\end{titlepage}

\begin{abstract}
Fault-detection in robotics systems is a difficult task and as systems
are becoming more larger and complex, subtle errors are becoming
harder to diagnose. Traditional fault-detection approaches have relied
on explicit modeling of component behavior, but this technique does
not scale to complex robots operating in dynamic environments. A new
technique which involves making the robot self-aware to the internal
state of its various components is examined. The aim of this project
is to implement and then measure the efficacy of this probabilistic
self-awareness model for the robotics middleware CAST
\cite{haweswyatt10aei}, and if time allows, deal with shortcomings of
the original approach.

\vspace{0.5cm}
\noindent\textit{Keywords}: robotics, fault detection,
machine learning
\end{abstract}
\newpage

\renewcommand{\abstractname}{Acknowledgments}
\begin{abstract}
Thanks Mum!
\end{abstract}
\newpage

\tableofcontents
\newpage
#+end_latex

* Motivation
:PROPERTIES:
:CUSTOM_ID: motivation
:END:
######################################################################
# Why is it important?
######################################################################
The number and uses of robots is increasing. More and more robots are
becoming part of the daily human experience.
# There are now robots which clean the house, assist in surgery, and
# automate the construction of goods.
There are robots in factories automating difficult, repetitive tasks.
There are an increasing number of domestic robots being used for
assistance in the home and as entertainment. There are even robots
being used as surgical tools. It is important for these robots to
function correctly, and if unable to do so, to degrade gracefully to
minimize harm to themselves and others. To do this, robots need some
way to determine their own operating conditions. Detecting faults
within robot systems is a hard problem.

The importance of equipping a robot with the ability of self-awareness
increases as humans interact more with robots. One could imagine a
robot in the home which assisted an elderly person or a robot
performing heart surgery. A malfunction in these situations a
malfunction could cause death or serious injury. One could also
imagine robotic arms at a factory building cars. A malfunction could
cause damage to products or the arm itself resulting in a loss of
factory output. These examples underscore the importance of detecting
and handling faults within a robot system.

######################################################################
# Who else has wanted to solve it and how did they do it?
######################################################################

######################################################################
# What was the goal of my project?
######################################################################
This project was conducted in collaboration with Raphael Golombek at
Bielefeld University. The main goal for the project was to take
Raphael's software, modify it to work with the robotics middleware
used at the Univeristy of Birmingham---the CoSY Architecture Schema
Toolkit---and determine its efficacy. To determine the efficacy of
this approach, a number of experiments were conducted. With the time
available after the completion of the main project component, the
scalability of the learned model was examined and alternative
formulations proposed.

######################################################################
# Introduce the rest of the dissertation
######################################################################
The rest of the dissertation is organized into six main sections. It
will begin with a literature review which gives a brief overview of
the techniques used by others who have wanted to solve this problem.
The literature review will also show where the technique presented in
this dissertation fits with previous techniques. Following on from the
literature revew, the theory with necessary background will be
covered. This section should give the reader enough understanding to
implement the algorithm. After an explanation of the theory, a section
will cover its implementation on CAST. This section will also give
some experimental results as well as asymptotic analysis of the
algorithm. The next few sections will cover alternative formulations
of the learned model, experimental results, as well as asymptotic
analysis. Finally, the last few sections will cover project management
and present an evaluation of the project.

* Literature review
:PROPERTIES:
:CUSTOM_ID: lit-review
:END:

The problem of closing-the-loop is a problem which has received
considerable attention since at least the 1980s \cite{deKleer:1987vc}.
There are two main approaches to solving this problem: model-based and
data-driven. Model-based approaches have been the domain of more
traditional engineering and are created through exhaustive
specification of component behavior. Data-driven approaches are newer
and leverage machine learning techniques.

Within the domain of model-based approaches, there exists two styles
of techniques. The first is the analytical approach which is used in
problems which are close to the hardware. They depend on a set of
assumptions, rigorous specification, and testing \cite{blanke2006}.
These analytical approaches also require the creation of recovery
routines for all possible faults. Because this approach is so tedious,
work has been made on trying to incorporate the data-driven with
analytical approaches \cite{Luo:2010ud}. The second is the
knowledge-based approach where qualitative models are used. A popular
method has been consistency-based diagnosis \cite{deKleer:1987vc},
which often make use of propositional logic. There has been much work
in this domain on diagnosis engines such as Livingstone
\cite{Kurien:2000ta,Williams:1996wf}, HyDE \cite{Narasimhan:2007ty},
and Lydia \cite{Feldman:2010uy}. The Livingstone has been very
successful, having been used on /Deep Space One/ \cite{Bajwa:2002tm},
/Earth Observing One/ \cite{Hayden:2004vn}, and on the /Autosub 6000/
\cite{Ernits:2010tm}.

Data-driven approaches have used machine learning to


The method used in this project was first described in
\cite{Golombek:2010hj} and \cite{Golombek:2011ek}. It is a purely
data-driven approach which finds structure in the temporal-stream of
communication between software components in a complex robotics
system. The original system was developed to work with the XCF
middleware and for this project has been extended to work the CAST
middleware.

** COMMENT Who has used the data-driven approach and what did they do?
*** (1) Learning a probabilistic error detection model for robotic systems
*** (18) Fault Detection and Diagnosis in Industrial Systems
*** (19) To reject or not to reject: that is the question-an answer in case of neural classifiers
*** (20) Data mining for cyber security
*** (21) A markov chain model of temporal behavior for anomaly detection
*** (22) Overcoming HMM time independence assumption using n-gram based modelling for continuous speech recognition
** COMMENT knowledge-based
*** (5)  Back to the future for consistency-based trajectory tracking
*** (8)  Diagnosing multiple faults
*** (9)  A model-based approach to reactive self-configuring systems
*** (10) Hyde - a general framework for stochastic and hybrid model-based diagnosis
*** (11) Approximation algorithms for model-based diagnosis
*** (12) The Livingstone model of a main propulsion system
*** (13) Lessons learned in the Livingstone 2 on Earth Observing One flight experiment
*** (14) Diagnosis of Autosub 6000 using automatically generated software models
*** (15) Combining particle filters and consistency-based approaches for monitoring and diagnosis of stochastic hybrid systems
*** (16) Diagnosis by a waiter and a Mars explorer
*** (17) Real-time diagnosis and repair of faults of robot control software
* Theory
:PROPERTIES:
:CUSTOM_ID: sec:theory
:END:
######################################################################
# Give a high-level idea & introduce the main theoretical steps
######################################################################
The main hypothesis of this approach states that a robot is a set of
communicating components which generate temporal communication
patterns when accomplishing tasks. These temporal communication
patterns exhibit structures which depend on the current state of the
robot \cite{Golombek:2010hj}. Because this approach uses a
machine-learned model, it falls completely within the data-driven
approach to fault detection as described in section \ref{lit-review}.

In order to classify the robot as being in a normal or faulty state, a
score is calculated against the learned model. This score is compared
against a moving threshold to create the classification of the robot's
state at any given time.

######################################################################
# Introduce the example to be used in explaining the idea
######################################################################

#+caption: In this simple example, it can be seen that.
#+label: fig:simple
[[file:img/simple.pdf]]

To ground the discussion, a simplistic example is shown in Figure
\ref{fig:ex1}. This graph represents a set of three components and how
messages pass through the system. These components can be seen as
chained together with a linear communication pattern. In this example,
node /A/ publishes a message /a/ at timestamp $t$ which passes to node
/B/. Node /B/, after doing some arbitrary computation, publishes a
message /b/ at timestamp $t'$ which is passed to node /C/. So, in this
example, it can be expected that data flow linearly from node /A/ to
node /C/. It is not necessary that data flow linearly through a
system. In general, real-life robotics systems exhibit more
complicated inter-component communication patterns. Figure
\ref{fig:ex2} shows a system in which node /A/ publishes two types of
messages, each of which is used by a separate component.

#+CAPTION:    A non-linear component-based system
#+LABEL:      fig:ex2
#+ATTR_LaTeX: width=0.5\textwidth
[[file:img/complex.pdf]]

The rest of this section will explain the theory using the examples of
figures \ref{fig:ex1} and \ref{fig:ex2} as examples. First, the idea
and creation of the learned model will be explained, followed by the
calculation of the score, and then the calculation of the final
classification.

** Learning the model

The learned model exploits the hypothesis that a robot composed of a
set of software components exhibits temporal communication patterns.
These patterns exhibit different structures depending on the state of
the robot. The goal then becomes to learn the inter-component
communication patterns when the robot is functioning correctly. With
this model, the robot's state can be classified depending on how
closely its current communication patterns adhere to the learned
communication patterns. If the current pattern deviates too far from
the learned pattern, then the robot can be said to be in an anomalous
state. The first step, then, is to create the learned model.


The model is learned by collecting an observation time-series and
learning how components publish with respect to one another. More
formally, let $E$ be the set of encoded time-series of component
communication data which is recorded during normal operation. For each
tuple $(e_i,e_j) \in E \times E$, a probability distribution
$P_{ij} = P(t \vert e_i,e_j)$ is estimated. The distribution $P_{ij}$
represents the probability the even $e_i$ occurs at timestamp $t_i$
and that after a delay of time $t$, event $e_j$ occurs (i.e., $t_j =
t_i + t$). The event $e_i$ is constrained to be the last seen
occurrence of this event type because the goal is to model temporal
correlations between the current event and the last seen occurrence of
a given event. Learning the model for the example present backed in
figure \ref{fig:ex1}, results in a matrix of distributions as shown
the in matrix in \eqref{matrix:ex1}.

\begin{equation}
\label{matrix:ex1}
\begin{bmatrix}
P_{aa} & P_{ab} & P_{ac}\\
P_{ba} & P_{bb} & P_{bc}\\
P_{ca} & P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}

It should be clear by now that the model does not learn transition
times between sets of connected components, but instead learns the
likelihood of the time-span between the firing of any two components.

The estimation of $P_{ij}$ makes use of Kernel Density estimators
which have been initialized with a Gaussian Kernel $K(u) =
\frac{1}{2\pi}e^{-\frac{1}{2}u^2}$. The set of all learned
distributions becomes the model $\mathcal{M} = \{P_{ij} \vert
(e_i,e_j) \in E \times E\}$. $\mathcal{M}$ is now the matrix shown in
\eqref{matrix:ex1}.

#+caption: A distribution is learned for each set of event types.
[[file:img/learned.pdf]]

** Calculating the score

During a live run, the score is calculated by comparing the incoming
stream of communication to the learned model. Essentially, the score
is higher the more closely the incoming pattern matches the learned
pattern. Formally, the score at event $e_j$ is defined as

\begin{equation}\label{eq:score}
s_j = \sum_{e_i \in E} w_{ij} \cdot P_{ij}(\Delta{}t_i)
\end{equation}

\noindent where $E$ is the set of last seen instance of each event
type and $w_{ij}$ is the relative weighting of the probability value.
The weight $w_{ij}$ is a measure of how meaningful the particular
distribution $P_{ij}$ is as an indication to the system's performance.
The weight is defined as

\begin{equation}\label{eq:weight}
w_{ij} = 1 - \frac{h_{ij}}{\sum_{e_i \in E} h_{ij}}
\end{equation}

The weight calculation presented in equation \eqref{eq:weight} makes use
of the entropy of the distribution. This represents how much
information is contained in a particular distribution and its
trustworthiness. Essentially, the lower the entropy, and thus the more
information contained in the distribution, the more willing we are to
trust the correlation between the two event types.

#+BEGIN_LATEX
\begin{algorithm}
\caption{Calculating the score on the receipt of event $e_j$ with
the set E of last seen instances of all event types.}
\label{alg:score}
\begin{program}
\FUNCT |score|(e_j, E) \BODY
|return | \lVert \sum_{e_i}^E (1 - \frac{h_{ij}}{H_j}) P_{ij}(\Delta(e_i,e_j)) \rVert
\WHERE
h_{ij} \equiv \text{ entropy of } P_{ij}
H_j    \equiv \text{ sum entropy of } P_{*j}
\Delta(i,j) \equiv \text{ timespan between events $i$ and $j$}
\END
\end{program}
\end{algorithm}
#+END_LATEX

** Calculating the threshold

An important aspect of this technique is that as the score changes
over the course of a system run, so does the threshold. What is
considered the threshold for normal behavior is dependent on the
communication patterns within the system. The threshold changes
according to formula \eqref{eq:threshold}. The idea behind this
formula is that variance $S_{var}$ of consecutive scores $S = (s_1,
\dotsm, s_{j-1}, s_j)$ is lower when events match the normal pattern
learned in the model $\mathcal{M}$. So, when the variance is lower, and thus the
events better match the learned model, the threshold is lowered. If
the score variance increases, the threshold increases as well to make
the threshold harder to exceed.

This formula is defined formally as

\begin{equation}\label{eq:threshold}
s^* = a \cdot s^*_{val} + (1 - a) \cdot s^*_{val} \cdot \frac{S_{var}}{s^*_{var}}
\end{equation}

where $S_{var}$ is the score variance, $s^*$ is the threshold
variance, and $s^*_{val}$ is a constant minimum threshold which is
determined before runtime.

** Classifying the system

With the score and threshold calculated, classifying the system is
straight forward. As can be seen in \eqref{eq:classification}, the
system is considered abnormal anytime the score of the current event
$e_j$ does not exceed the calculated threshold $s^*$.

\begin{equation}\label{eq:classification}
\text{abnormal}(e_j) = \begin{cases}
&\text{true}  : s_j < s^*\\
&\text{false} : else
\end{cases}
\end{equation}

* Original system
** Implementation

To implement the technique first specified by \cite{Golombek:2010hj}
on CAST, it was necessary to modify the source first implemented by
the original author and create a CAST component to connect to the
modified source. This section will cover the changes made, and the
background knowledge to put it into context, as well as the
description of the CAST component, also with the required background
knowledge.

######################################################################
# FTS
######################################################################
The original system create at Bielefeld was implemented using the
Filtering, Transformation, and Selection Library (FTS)
\cite{Luetkebohle09-FT}. Using FTS, one decomposes a problem into a
set of nodes which process data in pieces. This technique allows for
increased code re-usability

#+caption: The main steps shown in the FTS processing graph representation. Decomposing problems this way allows for high code re-use.
#+attr_latex: width=0.3\textwidth
[[file:img/fts.pdf]]


**** COMMENT What is the FTS graph processor?
- https://code.ai.techfak.uni-bielefeld.de/trac/xcf/wiki/FilterTransformSelect#topics
- https://toolkit.cit-ec.uni-bielefeld.de/components/tools/fts-filter-transform-select-toolkit
######################################################################
#  CAST
######################################################################
The CoSy Architecture Schema Toolkit (CAST) \cite{haweswyatt10aei} ...

**** COMMENT What is CAST?
**** COMMENT What did I have to do to make it work on CAST?
** Experimental results

In order to evaluate the system, a series of experiments were create
to test the algorithm. Three different CAST systems were created, each
with properties to push the algorithm (and the changes made to it) in
some way. In each of the following experiments, each component
publishes only a single event type.

*** Linear chain
:PROPERTIES:
:CUSTOM_ID: sec:3chain
:END:

**** Aim & Methodology

The linear chain was the simplest experiment run on the system. It is
the exact system presented in figure \ref{fig:simple}. This CAST setup
was used as a sanity check to ensure that the algorithm could function
on the simplest case. Failure to work on this case would mean that the
technique would likely not scale to larger systems.

The system was run with and without the induction of faults. The goal
was to see if the system could detect the induced fault without
flagging other states as fault. Data was collected and averaged over
10 runs to prevent skewering by an anomalous run.

**** Results


*** Parallel chains
:PROPERTIES:
:CUSTOM_ID: sec:4x4
:END:
**** Aim & Methodology

#+begin_src dot :exports none :file "img/4x4.pdf" :cache yes
digraph four_chain {
          rankdir=LR;
          A -> B -> C -> D;
          E -> F -> G -> H;
          I -> J -> K -> L;
          M -> N -> O -> P;
}
#+end_src
#+results[2c5c00e9891f5c001975c3b50767a7f5c481ed3c]:
[[file:img/4x4.pdf]]

#+label: fig:4x4
[[file:img/4x4.pdf]]

The aim of this experiment was to test the algorithm on a more complex
set of communication patterns. Figure \ref{fig:4x4} shows the layout
of the nodes within this system. The communication proceeded through
four separate linear chains---each chain completely independent of the
others.

As with the linear system described in section \ref{sec:3chain}, this
system was run with and without the induction of faults. The system
was run 10 times to ensure no anomalous experiment would skew
the final results.

**** Results

#+attr_latex: width=0.8\textwidth
[[file:./img/original_4x4.eps]]

Fault was detected 1.18 seconds after induction.

*** Non-connected components
:PROPERTIES:
:CUSTOM_ID: sec:10x1
:END:
**** Aim & Methodology

#+begin_src dot :exports none :file "img/10x0.pdf" :cache yes
  graph G {
          A;
          B; C; D; E; F; G; H; I; J;
  }
#+end_src
#+results[e12770e1913edc49ff97a14d956f8a319dd77a5a]:
[[file:img/10x0.pdf]]

This experiment was quite different from the others. It tested a
system in which none of the components communicated with one
another---a non-realistic system---in order to test the algorithm in
key ways. Similarly to the other experiments, this system was run 10
times with and without the induction of faults.

**** Results

#+attr_latex: width=0.8\textwidth
[[file:img/original_10x1.eps]]

Fault was detected 500 milliseconds after induction.

** Asymptotic analysis
:PROPERTIES:
:CUSTOM_ID: subsec:orig-asymp
:END:

When evaluating the approach first described in
\cite{Golombek:2010hj}, beyond knowing how it performed
experimentally, it was also desirable to know how the algorithm would
scale with input. This is done by performing asymptotic analysis of
the technique. It is the learned model which is truly core to this
approach and so analysis will focus on the model. There are two
aspects worth analyzing: runtime efficiency of calculating the score
from the model and space efficiency of the model itself.

Space efficiency is concerned with analyzing the amount of memory an
algorithm utilizes as input grows. In the approach described in
section \ref{sec:theory}, we saw that the algorithm learns a
probability distribution for the Cartesian product of the set of event
types. Because this value is constant, we can represent it formally as

\begin{equation}\label{eq:orig_memory}
\text{model}(n) \in  \Theta(n^2)
\end{equation}

This means that as the number of event types $n$ increases, the size
of the model must grow quadratically. During experimentation, it was
observed that with a system of 100 components, memory usage had
exceeded 4 GB.

The runtime efficiency of score calculation was another area of
concern because this algorithm depended directly on the size of the
model. The calculation will be based off of the algorithm
\ref{alg:score} from section \ref{sec:theory}. On analysis, we can see
that there are two aspects to the algorithm: calculating the sum
entropy and then calculating the whole score which can be seen in
equation \eqref{eq:orignal_score}.

#+BEGIN_LATEX
\begin{equation}
\label{eq:orignal_score}
\begin{split}
score(n) &= H_{ij} + \sum_{e_i}^E\\
score(n) &= n + 5n\\
score(n) &= 6n\\
score(n) &\in \Theta(n)
\end{split}
\end{equation}
#+END_LATEX

Since the sum entropy $H_j$ will be the same for all events $e_i \in
E$ on the receipt of event $e_j$, this only needs to be calculated
once. Calculating this value requires a simple summation over the $n$
entries which have information about the event type $j$, hence its
value is $n$. Similarly, the score calculation is a summation over the
$n$ relevant entries in $E$ with the addition of five steps for each
entry, hence $5n$. Performing arithmetic, we can see that while the
model may be $\Theta(n^2)$, the score calculation is only $\Theta(n)$
because it only considers the relevant entries.

* Connection-based model
** Idea & Implementation

The asymptotic analysis from section \ref{subsec:org-asymp} showed
that the space efficiency of the learned model could not scale with
larger systems. The goal for the rest of the project then became to
find a way to reduced the size of this model. The initial idea was to
use the information about the connections between components---all
information which could be gathered /a priori/ on CAST
\cite{Otto:2010uc}. This information could then be exploited to prune
the model and retain only the parts of the model which correlate to
actual paths of communication within the real system.

Using the example presented in figure \ref{fig:simple}, we can see
that informaton flows from node /A/ to node /B/ and from node /B/ to
node /A/. Using the idea of pruning, we could remove from the model
the learned distributions between nodes /A/ and /C/. Additionally,
since information in this example flows as a directed graph, we can
prune all distributions which correlate to the reverse direction,
e.g., $P_{ab}$. It was decided that the distribution which modeled a
component to itself would be kept because it would be useful to have a
distribution of how often a component fired. All together, the model
$\mathcal{M}$ is reduced to

#+BEGIN_LATEX
\begin{equation}
\label{eq:reduced_model}
\begin{bmatrix}
P_{aa} & \empty & \empty\\
P_{ba} & P_{bb} & \empty\\
\empty& P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}
#+END_LATEX

** Asymptotic analysis

The change in the formulation of the model affects how the size scales
with new event types. Analyzing the space efficiency of this approach,
we can see that in the worst case the system will be fully-connected.
The best case occurs when the system contains no connection between
any components. Formally, the space efficiency of this model is

#+BEGIN_LATEX
\begin{equation}
\begin{split}
\label{eq:reduced_asymp}
model(n) &\in O(n^2)\\
model(n) &\in \Omega(n)
\end{split}
\end{equation}
#+END_LATEX

** Experimental results
*** Linear chain
**** Aim & Methodology
**** Results
*** Parallel chains
**** Aim & Methodology
**** Results



*** Non-connected components
**** Aim & Methodology
**** Results
* Metronome-based approach
** Idea & Implementation

After the failure of the connection-based approach to reduce the model
and remain performant, a new approach had to be created. What was
created was based on the idea of a metronome, or a heartbeat, and how
it fires at a constant rate. By learning how every other component
fired relative to the metronome, it might be possible to dramatically
reduce the model size while still remaining performant.

To implement, this meant adding an extra component in the CAST system
and pruning all distributions which did not have the metronome =m= in
the $j$ position of a distribution $P_{ij}$. Performing this
optimization example shown in figure \ref{fig:simple} resulted in a
model $\mathcal{M}$ reduced to

#+begin_latex
\begin{equation}
\label{eq:metronome_model}
\begin{bmatrix}
P_{am} & P_{bm} & P_{cm} & P_{mm}
\end{bmatrix}
\end{equation}
#+end_latex

** Asymptotic analysis

Implementing this technique resulted in a far smaller model. Formally,
the space efficiency of this new model became

#+begin_latex
\begin{equation}
\label{eq:metronome_asymp}
model(n) \in \Theta(n + 1)
\end{equation}
#+end_latex

This difference results in a rather dramatic reduction. For example,
on complex CAST system with 100 components, the model size for the
original implementation would be $model(n) \in \Theta(n^2) = 10,000$.
With the metronome approach, the space efficiency for this same system
becomes $model(n) \in \Theta(n + 1) = 101$. The difference in space
efficiency means that the metronome approach could scale more than the
original implementation.

** Experimental results
*** Linear chain
**** Aim & Methodology
**** Results
*** Parallel chains
**** Aim & Methodology
**** Results

[[file:img/metronome_4x4.eps]]

*** Non-connected components
**** Aim & Methodology
**** Results

[[file:img/metronome_10x1.eps]]

* Project management

Large projects are strenuous. Effective project management then
becomes crucial in ensuring constant progress throughout academic
year.

Git was used rather than Subversion for one key reason: it is easy to
maintain multiple branches of the code and move changes to all of
them. This feature was especially important because it meant that
multiple ideas about the model implementation could be kept in
separate branches. In Subversion, doing the equivalent would have made
it very difficult to make updates to all branches when bugs were found
and fixed.

Because inheriting such a large code-base can be overwhelming, unit
tests were used to create a contract of behavior for the most critical
classes in the system. And by using Jenkins as a continuous
integration server, it was possible to know when any change to the
code caused a test on any branch to fail. Jenkins also published the
results of static analysis run by Maven, the build system used. Static
analysis helped suss out potential bugs and resulted in more robust code.

Perhaps the most important aspect of project management, and
unfortunately discovered only towards the end of the project, was
issue management. It was possible to set project milestones and attach
the issues necessary to complete the milestone. This has the benefit
of putting in concrete terms the steps necessary to reach a goal. So
rather than flailing around to figure out what to do next, there was
always a concrete task that could be done.

** COMMENT Managing tasks & deadlines
*** Github issues
** COMMENT Managing code
*** maven
*** Jenkins
*** git
* Project evaluation
** COMMENT What was good?
*** project planning w.r.t. summer work
** COMMENT What can be learned?
*** sticking with it when intial results are bad
*** setting better goals
* Conclusion
#+begin_comment
#+begin_quote
: In this paper we presented the on-line application of our fault
: detection approach for robotic systems. It is purely data-driven and
: exploits generic information extracted from the systemâ€™s
: inter-component communication. The conducted experiments demonstrated
: that our approach is capable of detecting and tracking various induced
: faults on-line with high probability and acceptable delay. Initially,
: the algorithm suffers form false positives but introducing another
: decision- layer based on a moving average reduces the false positive
: rate. Next steps will involve additional experiments to increase the
: significance of the results and evaluation of novel fault patterns.
: Furthermore, we will extend the detection model with sub-models
: representing different states of the system. This will involve
: different states of normal behaviour as well as states for already
: experienced faults. By this means we will on the one hand improve the
: fault detection and on the other hand enable fault diagnosis for
: experienced faults.
#+end_quote

#+begin_quote
: In this paper we presented a novel method for fault detection in
: robotic systems constructed on the basis of discrete event-based data
: interchange. The introduced self- awareness model is strongly data
: driven and thus (i) can be trained from good examples of normal system
: behavior, (ii) is largely independent from specific scenarios and
: (iii) shows promising results even for transient malfunctions in
: system behavior. The resulting self-awareness model provides a basis
: for a sophisticated autonomic computing architecture in the domain of
: robotic systems, enhancing safety and robustness of robot operation
: and ultimatively increasing the autonomoy of intelligent robot
: systems. Future work will focus on a broader and more realistic daily
: life evaluation as well as an exhaustive complexity and performance
: analysis of the demonstrated approach. Over a long distance our goal
: will be closing the autonomic control loop, effectively allowing
: modification of relevant system properties upon detected anomalies,
: the integration of anomaly detection with behavioral control and the
: further exploration of fault diagnosis models for robotic
: applications.
#+end_quote
#+end_comment

\newpage
\bibliographystyle{plain}
\bibliography{references}



#  LocalWords:  analytical middleware performant
* Graphs                                                           :NOEXPORT:
** Original
*** Linear
#+begin_comment
#+begin_src gnuplot :var data="./data/original-eps-converted-to" :exports none :file "img/reduced_3chain_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src
#+end_comment
*** 4x4

#+begin_src gnuplot :var norm="./data/original_4x4_normal.csv" fault="./data/original_4x4_fault.csv" :exports none :file "img/original_4x4.eps" :cache yes :tangle "data/original_4x4.plt"
  reset
  unset term
  set term postscript eps color solid eps enhanced 20
  set multiplot
  set yrange [0:1]
  set xrange [0:150000]
  set size 0.5, 1.0

  set title 'Normal'
  set origin 0.0, 0.0
  plot norm using 1:2 with dots notitle,\
       norm using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       norm using 1:3 with lines title 'Threshold'
  set title 'Fault'
  set origin 0.5, 0.0
  plot fault using 1:2 with dots notitle,\
       fault using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       fault using 1:3 with lines title 'Threshold'

#+end_src

#+RESULTS[7cd4153226f45a21b472dd5400d2db3560a33f9b]:
[[file:img/original_4x4.eps]]

*** 4x4 normal

#+begin_src gnuplot :var data="./data/original_4x4_normal.csv" :exports none :file "img/original_4x4_normal.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]

  set title 'Normal'
  plot norm using 1:2 with dots notitle,\
       norm using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       norm using 1:3 with lines title 'Threshold'
#+end_src

#+RESULTS[712afaa890dd2a697fe7b6fcc85e9d8f3528686f]:
[[file:img/original_4x4_normal.eps]]

*** 4x4 fault

#+begin_src gnuplot :var data="./data/original_4x4_fault.csv" :exports none :file "img/original_4x4_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  set title "Normal"
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src
#+RESULTS[43081e236f08b1ee98a8982967e878c0ad9f7e27]:
[[file:img/original_4x4_fault.eps]]

* Dot                                                              :NOEXPORT:
** Simple example

#+begin_src dot :exports none :file "img/simple.pdf" :cache yes
  digraph Example1 {
  rankdir=LR;
  subgraph cluster2 {
  label="Event from B";
  A3[label="A"];
  B3[label="B"];
  C3[label="C"];
  A3 -> B3
  [label="a  "];
  B3 -> C3
  [label="b  (150ms)",color="red",style="bold",fontcolor="red"];

  }
  subgraph cluster1 {
  label="Event from A";
  A2[label="A"];
  B2[label="B"];
  C2[label="C"];
  A2 -> B2
  [label="a  (100ms)",color="red",style="bold",fontcolor="red"];
  B2 -> C2 [label="b  "];
  }
  subgraph cluster0 {
  label="No event";
  A1[label="A"];
  B1[label="B"];
  C1[label="C"];
  A1 -> B1 [label="a  "];
  B1 -> C1 [label="b  "];
  }
  }
#+end_src
#+results[fc8897caa2a034eb34782fd9c83ca4451bb52636]:
[[file:img/simple.pdf]]

** Complex example

#+begin_src dot :exports none :file "img/complex.pdf" :cache yes
  digraph real {
  rankdir=LR;
  A -> B [dir="both"];
  A -> C [dir="both"];
  A -> D [dir="both"];
  A -> E [dir="both"];
  A -> F [dir="both"];
  B -> E;
  C -> D;
  D -> E;
  F -> D;
  }
#+end_src

#+results[8033868d33a97b13559b13165338665ffeaaf6df]:
[[file:img/complex.pdf]]

** Learned

#+begin_src dot :exports none :file "img/learned.pdf" :cache yes
  digraph G {
          rankdir=LR;
          A -> A [label="P(AA)"];
          A -> B [label="P(AB)"];
          A -> C [label="P(AC)"];
          B -> A [label="P(BA)"];
          B -> B [label="P(BB)"];
          B -> C [label="P(BC)"];
          C -> A [label="P(CA)"];
          C -> B [label="P(CB)"];
          C -> C [label="P(CC)"];
  }
#+end_src

#+RESULTS[cc2bb741e8fa3d5e6be7049aa932a42ec96640c5]:
[[file:img/learned.pdf]]
** FTS graph

#+begin_src dot :exports none :file "img/fts.pdf" :cache yes
  digraph G {
  CAST;
  CalcScore [label="Calculate Score"];
  ClassifyScore [label="Classify"];
  CAST -> Encode -> CalcScore -> ClassifyScore;
  ClassifyScore -> CAST [style="dotted"];
  }
#+end_src

#+RESULTS[29479010baef6dfc79c12ac1a41b34a2420b283b]:
[[file:img/fts.pdf]]
