#+title:
#+author:  Jeremiah M. Via
#+options: H:2 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+startup: hidestars indent
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: dissertation
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]

#+begin_latex
\begin{titlepage}
%% Set the line spacing to 1 for the title page.
\begin{spacing}{1}
\begin{large}
\begin{center}
\mbox{}
\vfill
\begin{sc}
A Data-Driven Self-Awareness Model for Robotics Systems \\
\end{sc}
\vfill
Jeremiah M. Via \\
Supervisor: Nick Hawes \\
\vspace*{4mm}
\includegraphics[width=50mm]{crest.png}\\
Submitted in conformity with the requirements\\
for the degree of Artificial Intelligence \& Computer Science\\
School of Computer Science\\
University of Birmingham\\
\vfill
Copyright {\copyright} 2012 School of Computer Science, University of Birmingham\\
\vspace*{.2in}
\end{center}
\end{large}
\end{spacing}
\end{titlepage}

\begin{abstract}
Fault-detection in robotics systems is a difficult task and as systems
are becoming more larger and complex, subtle errors are becoming
harder to diagnose. Traditional fault-detection approaches have relied
on explicit modeling of component behavior, but this technique does
not scale to complex robots operating in dynamic environments. A new
technique which involves making the robot self-aware to the internal
state of its various components is examined. The aim of this project
is to implement and then measure the efficacy of this probabilistic
self-awareness model for the robotics middleware CAST
\cite{haweswyatt10aei}, and if time allows, deal with shortcomings of
the original approach.

\vspace{0.5cm}
\noindent\textit{Keywords}: robotics, fault detection,
machine learning
\end{abstract}
\newpage

\renewcommand{\abstractname}{Acknowledgments}
\begin{abstract}
Thanks Mum!
\end{abstract}
\newpage

\tableofcontents
\newpage
#+end_latex

* Motivation
:PROPERTIES:
:CUSTOM_ID: motivation
:END:
######################################################################
# Why is it important?
######################################################################
The number and uses of robots is increasing. More and more robots are
becoming part of the daily human experience.
# There are now robots which clean the house, assist in surgery, and
# automate the construction of goods.
There are robots in factories automating difficult, repetitive tasks.
There are an increasing number of domestic robots being used for
assistance in the home and as entertainment. There are even robots
being used as surgical tools. It is important for these robots to
function correctly, and if unable to do so, to degrade gracefully to
minimize harm to themselves and others. To do this, robots need some
way to determine their own operating conditions. Detecting faults
within robot systems is a hard problem.

The importance of equipping a robot with the ability of self-awareness
increases as humans interact more with robots. One could imagine a
robot in the home which assisted an elderly person or a robot
performing heart surgery. A malfunction in these situations a
malfunction could cause death or serious injury. One could also
imagine robotic arms at a factory building cars. A malfunction could
cause damage to products or the arm itself resulting in a loss of
factory output. These examples underscore the importance of detecting
and handling faults within a robot system.

######################################################################
# Who else has wanted to solve it and how did they do it?
######################################################################

######################################################################
# What was the goal of my project?
######################################################################
This project was conducted in collaboration with Raphael Golombek at
Bielefeld University. The main goal for the project was to take
Raphael's software, modify it to work with the robotics middleware
used at the Univeristy of Birmingham---the CoSY Architecture Schema
Toolkit---and determine its efficacy. To determine the efficacy of
this approach, a number of experiments were conducted. With the time
available after the completion of the main project component, the
scalability of the learned model was examined and alternative
formulations proposed.

######################################################################
# Introduce the rest of the dissertation
######################################################################
The rest of the dissertation is organized into six main sections. It
will begin with a literature review which gives a brief overview of
the techniques used by others who have wanted to solve this problem.
The literature review will also show where the technique presented in
this dissertation fits with previous techniques. Following on from the
literature revew, the theory with necessary background will be
covered. This section should give the reader enough understanding to
implement the algorithm. After an explanation of the theory, a section
will cover its implementation on CAST. This section will also give
some experimental results as well as asymptotic analysis of the
algorithm. The next few sections will cover alternative formulations
of the learned model, experimental results, as well as asymptotic
analysis. Finally, the last few sections will cover project management
and present an evaluation of the project.

* Literature review
:PROPERTIES:
:CUSTOM_ID: lit-review
:END:

The problem of closing-the-loop with respect to robots is a problem
which has received considerable attention since at least the 1980s
\cite{deKleer:1987vc}. There are two main approaches to solving this
problem: model-based and data-driven. Model-based approaches have been
the domain of more traditional engineering and are created through
exhaustive specification of component behavior. Data-driven approaches
are newer and leverage machine learning techniques.

** Who has used the model-based approach and what did they do?
*** analytic
- "used in the design of control systems where the models are
  constructed based on fundamental assumptions"
- time consuming, so not much use in the complex cognitive systems
  of modern robots
**** (6) Diagnosis and Fault-Tolerant Control
**** (7) Integrated model-based and data-driven diagnosis of automotive antilock braking systems
*** knowledge-based
- "qualitative models of the system are used to
  detect and diagnose faults"
- require expert knowledge to design the model
**** (5)  Back to the future for consistency-based trajectory tracking
**** (8)  Diagnosing multiple faults
**** (9)  A model-based approach to reactive self-configuring systems
**** (10) Hyde - a general framework for stochastic and hybrid model-based diagnosis
**** (11) Approximation algorithms for model-based diagnosis
**** (12) The Livingstone model of a main propulsion system
**** (13) Lessons learned in the Livingstone 2 on Earth Observing One flight experiment
**** (14) Diagnosis of Autosub 6000 using automatically generated software models
**** (15) Combining particle filters and consistency-based approaches for monitoring and diagnosis of stochastic hybrid systems
**** (16) Diagnosis by a waiter and a Mars explorer
**** (17) Real-time diagnosis and repair of faults of robot control software
** Who has used the data-driven approach and what did they do?
*** (1) Learning a probabilistic error detection model for robotic systems
*** (18) Fault Detection and Diagnosis in Industrial Systems
*** (19) To reject or not to reject: that is the question-an answer in case of neural classifiers
*** (20) Data mining for cyber security
*** (21) A markov chain model of temporal behavior for anomaly detection
*** (22) Overcoming HMM time independence assumption using n-gram based modelling for continuous speech recognition
** How does aucom fit in with these approaches?
Aucom is a purely data-driven approach.

* Theory
######################################################################
# Give a high-level idea & introduce the main theoretical steps
######################################################################
The main hypothesis of this approach states that a robot is a set of
communicating components which generate temporal communication
patterns when accomplishing tasks. These temporal communication
patterns exhibit structures which depend on the current state of the
robot \cite{Golombek:2010hj}. Because this approach uses a
machine-learned model, it falls completely within the data-driven
approach to fault detection as described in section \ref{lit-review}.

In order to classify the robot as being in a normal or faulty state, a
score is calculated against the learned model. This score is compared
against a moving threshold to create the classification of the robot's
state at any given time.

######################################################################
# Introduce the example to be used in explaining the idea
######################################################################
#+CAPTION:    A simple component-based system
#+LABEL:      fig:ex1
#+ATTR_LaTeX: width=0.7\textwidth
[[./diagrams/ex1.pdf]]

To ground the discussion, a simplistic example is shown in Figure
\ref{fig:ex1}. This graph represents a set of three components and how
messages pass through the system. These components can be seen as
chained together with a linear communication pattern. In this example,
node /A/ publishes a message /a/ at timestamp $t$ which passes to node
/B/. Node /B/, after doing some arbitrary computation, publishes a
message /b/ at timestamp $t'$ which is passed to node /C/. So, in this
example, it can be expected that data flow linearly from node /A/ to
node /C/. It is not necessary that data flow linearly through a
system. In general, real-life robotics systems exhibit more
complicated inter-component communication patterns. Figure
\ref{fig:ex2} shows a system in which node /A/ publishes two types of
messages, each of which is used by a separate component.

#+CAPTION:    A non-linear component-based system
#+LABEL:      fig:ex2
#+ATTR_LaTeX: width=0.5\textwidth
[[./diagrams/ex2.pdf]]

The rest of this section will explain the theory using the examples of
figures \ref{fig:ex1} and \ref{fig:ex2} as examples. First, the idea
and creation of the learned model will be explained, followed by the
calculation of the score, and then the calculation of the final
classification.

** Learning the model

The learned model exploits the hypothesis that a robot composed of a
set of software components exhibits temporal communication patterns.
These patterns exhibit different structures depending on the state of
the robot. The goal then becomes to learn the inter-component
communication patterns when the robot is functioning correctly. With
this model, the robot's state can be classified depending on how
closely its current communication patterns adhere to the learned
communication patterns. If the current pattern deviates too far from
the learned pattern, then the robot can be said to be in an anomalous
state. The first step, then, is to create the learned model.


The model is learned by collecting an observation time-series and
learning how components publish with respect to one another. More
formally, let $E$ be the set of encoded time-series of component
communication data which is recorded during normal operation. For each
tuple $(e_i,e_j) \in E \times E$, a probability distribution
$P_{ij}=P(t|e_i,e_j)$ is estimated. The distribution $P_{ij}$
represents the probability the even $e_i$ occurs at timestamp $t_i$
and that after a delay of time $t$, event $e_j$ occurs (i.e., $t_j =
t_i + t$). The event $e_i$ is constrained to be the last seen
occurrence of this event type because the goal is to model temporal
correlations between the current event and the last seen occurrence of
a given event. Learning the model for the example present backed in
figure \ref{fig:ex1}, results in a matrix of distributions as shown in
equation \ref{matrix:ex1}.

\begin{equation}
\label{matrix:ex1}
\begin{bmatrix}
P_{aa} & P_{ab} & P_{ac}\\
P_{ba} & P_{bb} & P_{bc}\\
P_{ca} & P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}

It should be clear by now that the model does not learn transition
times between sets of connected components, but instead learns the
likelihood of the time-span between the firing of any two components.

The estimation of $P_{ij}$ makes use of Kernel Density estimators
which have been initialized with a Gaussian Kernel $K(u) =
\frac{1}{2\pi}e^{-\frac{1}{2}u^2}$. The set of all learned
distributions becomes the model $M = \{P_{ij}|(e_i,e_j) \in E \times
E\}$. $M$ is now the matrix shown in equation \ref{matrix:ex1}.

** Calculating the score

During a live run, the score is calculated by comparing the incoming
stream of communication to the learned model. Essentially, the score
is higher the more closely the incoming pattern matches the learned
pattern. Formally, the score at event $e_j$ is defined as

\begin{equation}
\label{eq:score}
  s_j = \sum_{e_i \in E} w_{ij} \cdot P_{ij}(\Delta{}t_i)
\end{equation}

\noindent where $E$ is the set of last seen instance of each event
type and $w_{ij}$ is the relative weighting of the probability value.
The weight $w_{ij}$ is a measure of how meaningful the particular
distribution $P_{ij}$ is as an indication to the system's performance.
The weight is defined as

\begin{equation}
\label{eq:weight}
w_{ij} = 1 - \frac{h_{ij}}{{\displaystyle\sum_{e_i \in E} h_{ij}}}
\end{equation}

The weight calculation presented in equation \ref{eq:weight} makes use
of the entropy of the distribution. This represents how much
information is contained in a particual distribution and its
trustworthiness. Essentially, the lower the entropy, and thus the more
information contained in the distribution, the more willing we are to
trust the correlation between the two event types.

** Calculating the threshold

An important aspect of this technique is that as the score changes
over the course of a system run, so does the threshold. What is
considered the threshold for normal behavior is dependent on the
communication patterns within the system. 

** Classifying the system
* Original system
** Implementation
*** CAST
**** What is CAST?
**** What did I have to do to make it work on CAST?
*** FTS graph
**** What is the FTS graph processor?
- https://code.ai.techfak.uni-bielefeld.de/trac/xcf/wiki/FilterTransformSelect#topics
- https://toolkit.cit-ec.uni-bielefeld.de/components/tools/fts-filter-transform-select-toolkit
** Experimental results
*** introduce the experiments for the rest of the report
**** record fault tracking time
*** 3x1 experiments

#+CAPTION:    The black-body emission of the disk around HR 4049
#+LABEL:      fig:SED-HR4049
#+ATTR_LaTeX: width=0.8\textwidth
[[./data/original.eps]]
*** 4x4 experiment results
*** 10x1 experiment results
*** dora experiment
** Asymptotic analysis
*** Show mathematical derivation of model memory
*** Show mathematical dervaition of score calculation
* Connection-based model
** Idea
** Implementation
** Asymptotic analysis
*** model
*** score calculation
** Experimental results
*** 3x1 experiments

#+begin_src gnuplot :var data="./data/reduced_3chain_fault" :exports none :file "img/reduced_3chain_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src

#+results[9ccf3b2dca2bb91beb98ba4a1be6a9b33c7b1112]:
[[file:img/reduced_3chain_fault.eps]]



*** 4x4 experiment results

The results of the experiment.

#+begin_src gnuplot :var data="./data/reduced_4x4_normal.csv" :exports none :file "img/reduced_4x4_normal.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src

#+results[fb1cd17f2e0a6b3fffda70ab3cd1dfad0f703b48]:
[[file:img/reduced_4x4_normal.eps]]


#+begin_src gnuplot :var data="./data/reduced_4x4_fault.csv" :exports none :file "img/reduced_4x4_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src

#+results[b9082141948d6b6bb2e672f95ac3d7612c19a98f]:
[[file:img/reduced_4x4_fault.eps]]


*** 10x1 experiment results

The results of the experiment.

#+begin_src gnuplot :var data="./data/reduced_10x1_fault.csv" :exports results :file "img/reduced_10x1_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src

#+results[02d0464828433875ba8f7fe252783f2d70fcdfed]:
[[file:img/reduced_10x1_fault.eps]]

*** dora experiment
* Metronome-based approach
** Idea
** Implementation
** Asymptotic analysis
*** model
*** score calculation
** Experimental results
*** 3x1 experiments
*** 4x4 experiment results
*** 10x1 experiment results
*** dora experiment
*** ROC analysis of the three approaches
* Project management
Large projects are strenuous. Effective project management then
becomes crucial in ensuring constant progress throughout all periods
of the academic year.

Git was used rather than Subversion for one key reason: it is easy to
maintain multiple branches of the code and move changes to all of
them. This feature was especially important because it meant that
multiple ideas about the model implementation could be kept in
separate branches. In Subversion, doing the equivalent would have made
it very difficult to make updates to all branches when bugs were found
and fixed.

Because inheriting such a large code-base can be overwhelming, unit
tests were used to create a contract of behavior for the most critical
classes in the system. And by using Jenkins as a continuous
integration server, it was possible to know when any change to the
code caused a test on any branch to fail. Jenkins also published the
results of static analysis run by Maven, the build system used. Static
analysis helped suss out potential bugs and resulted in more robust code.

Perhaps the most important aspect of project management, and
unfortunately discovered only towards the end of the project, was
issue management. It was possible to set project milestones and attach
the issues necessary to complete the milestone. This has the benefit
of putting in concrete terms the steps necessary to reach a goal. So
rather than flailing around to figure out what to do next, there was
always a concrete task that could be done.

** Managing tasks & deadlines
*** Github issues
** Managing code
*** maven
*** Jenkins
*** git
* Project evaluation
** What was good?
*** project planning w.r.t. summer work
** What can be learned?
*** sticking with it when intial results are bad
*** setting better goals
* Conclusion
** Conclude story
** Future work


\newpage
\bibliographystyle{plain}
\bibliography{references}


