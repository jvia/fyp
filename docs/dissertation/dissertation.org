#+title:
#+author:  Jeremiah M. Via
#+options: H:4 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+startup: hidestars indent
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LATEX_CLASS: dissertation
#+LATEX_CLASS_OPTIONS: [a4paper,oneside,12pt,onecolumn,final,openany]
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{program}
#+LATEX_HEADER: \NumberProgramstrue

#+begin_latex
\begin{titlepage}
%% Set the line spacing to 1 for the title page.
\begin{spacing}{1}
\begin{large}
\begin{center}
\mbox{}
\vfill
\begin{sc}
A Data-Driven Self-Awareness Model for Robotics Systems \\
\end{sc}
\vfill
Jeremiah M. Via \\
Supervisor: Nick Hawes \\
\vspace*{4mm}
\includegraphics[width=50mm]{crest.png}\\
Submitted in conformity with the requirements\\
for the degree of Artificial Intelligence \& Computer Science\\
School of Computer Science\\
University of Birmingham\\
\vfill
Copyright {\copyright} 2012 School of Computer Science, University of Birmingham\\
\vspace*{.2in}
\end{center}
\end{large}
\end{spacing}
\end{titlepage}

\begin{abstract}
Fault-detection in robotics systems is a difficult task and as systems
are becoming more larger and complex, subtle errors are becoming
harder to diagnose. Traditional fault-detection approaches have relied
on explicit modeling of component behavior, but this technique does
not scale to complex robots operating in dynamic environments. A new
technique which involves making the robot self-aware to the internal
state of its various components is examined. The aim of this project
is to implement and then measure the efficacy of this probabilistic
self-awareness model for the robotics middleware CAST
\cite{haweswyatt10aei}, and if time allows, deal with shortcomings of
the original approach.

\vspace{0.5cm}
\noindent\textit{Keywords}: robotics, fault detection,
machine learning
\end{abstract}
\newpage

\renewcommand{\abstractname}{Acknowledgments}
\begin{abstract}
Thanks Mum!
\end{abstract}
\newpage

\tableofcontents
\newpage
#+end_latex

* Motivation
:PROPERTIES:
:CUSTOM_ID: motivation
:END:
######################################################################
# Why is it important?
######################################################################
The number and uses of robots is increasing. More and more robots are
becoming part of the daily human experience.
# There are now robots which clean the house, assist in surgery, and
# automate the construction of goods.
There are robots in factories automating difficult, repetitive tasks.
There are an increasing number of domestic robots being used for
assistance in the home and as entertainment. There are even robots
being used as surgical tools. It is important for these robots to
function correctly, and if unable to do so, to degrade gracefully to
minimize harm to themselves and others. To do this, robots need some
way to determine their own operating conditions. Detecting faults
within robotics systems is a hard problem.

The importance of equipping a robot with the ability of self-awareness
increases as humans interact more with robots. One could imagine a
domestic robot which assisted an elderly person or a robot performing
heart surgery. A malfunction in these situations could cause death or
serious injury. One could also imagine robotic arms at a factory
building cars where a malfunction could cause damage to products or
the arm itself resulting in a loss of factory output. These examples
underscore the importance of detecting and handling faults within a
robot system.

######################################################################
# What was the goal of my project?
######################################################################
This project was conducted in collaboration with Raphael Golombek at
Bielefeld University who first implemented this technique on the XCF
middleware \cite{Wrede:2004th}. The main goal for the project was to
take Raphael's software, modify it to work with the robotics
middleware used at the Univeristy of Birmingham---the CoSY
Architecture Schema Toolkit (CAST)---and determine its efficacy. The
impetus behind this was to see if the technique developed by Raphael
could work on another middleware. To determine the efficacy of this
approach, a number of experiments were conducted to test the
algorithm's capabilities. With the time available after the completion
of the main project component, the scalability of the learned model
was examined and alternative formulations proposed.

######################################################################
# Introduce the rest of the dissertation
######################################################################
The rest of the dissertation is organized into six main sections. It
will begin with a literature review which gives a brief overview of
the techniques used by others who have attempted to solve this
problem. The literature review will also show where the technique
presented in this dissertation fits with previous techniques.
Following on from the literature revew, the theory section will cover
the necessary background to understanding the technique presented in
\cite{Golombek:2011ek}. This section should give the reader enough
understanding to implement the technique on any event-based robotics
middleware. After an explanation of the theory, a section will cover
its implementation on CAST. This section will also give some
experimental results as well as asymptotic analysis of the key
portions of the technique. The asymptotic analysis will be an
important motivation for the alternative model formulations presented
in later sections. After covering the original implementation and its
analysis, the next few sections will cover alternative formulations of
the learned model, experimental results, as well as asymptotic
analysis. The differences between these model formulations and the
trade-offs required will be discussed. Finally, the last few sections
will cover project management and present an evaluation of the
project.

* Literature review
:PROPERTIES:
:CUSTOM_ID: lit-review
:END:

The problem of closing-the-loop is a problem which has received
considerable attention since at least the 1980s \cite{deKleer:1987vc}.
There are two main approaches to solving this problem: model-based and
data-driven. Model-based approaches have been the domain of more
traditional engineering and are created through exhaustive
specification of component behavior. Data-driven approaches are newer
and leverage machine learning techniques.

Within the domain of model-based approaches, there exists two styles
of techniques. The first is the analytical approach which is used in
problems which are close to the hardware. They depend on a set of
assumptions, rigorous specification, and testing \cite{blanke2006}.
These analytical approaches also require the creation of recovery
routines for all possible faults. Because this approach is so tedious,
work has been made on trying to incorporate the data-driven with
analytical approaches \cite{Luo:2010ud}. The second is the
knowledge-based approach where qualitative models are used. A popular
method has been consistency-based diagnosis \cite{deKleer:1987vc},
which often make use of propositional logic. There has been much work
in this domain on diagnosis engines such as Livingstone
\cite{Kurien:2000ta,Williams:1996wf}, HyDE \cite{Narasimhan:2007ty},
and Lydia \cite{Feldman:2010uy}. The Livingstone has been very
successful, having been used on /Deep Space One/ \cite{Bajwa:2002tm},
/Earth Observing One/ \cite{Hayden:2004vn}, and on the /Autosub 6000/
\cite{Ernits:2010tm}.

Data-driven approaches have used machine learning in an attempt to
bypass the effort involved with model-based approaches. A wide variety
of approaches have been tried, broadly categorized into deterministic
and stochastic \cite{Golombek:2011ek}. Deterministic approaches have
generally attempted to cluster the feature space to separate normal
classes from faulty ones \cite{DeStefano:2000vt,Chandola:2006um}.
Stochastic techniques have assumed that anomalous data points would be
in low probability areas \cite{Casar:2008tp,Ye:2000uu}. The idea then
is to use statistical inference on model which has been fit to
previously gathered data. Generally, the deterministic approaches have
failed to handle the uncertainty present in the complex systems in
robots.The stochastic approaches previously mentioned have depended on
the Markov assumption which does not hold in the complex communication
patterns in a robot \cite{Golombek:2010hj}.

The method used in this project was first described in
\cite{Golombek:2010hj} and \cite{Golombek:2011ek}. It is a purely
data-driven approach which finds structure in the temporal-stream of
communication between software components in a complex robotics
system. The original system was developed to work with the XCF
middleware and for this project has been extended to work the CAST
middleware.

** COMMENT 2010 paper
*** (9) Reliable detection of episodes in event sequences
** COMMENT Who has used the data-driven approach and what did they do?
*** (1) Learning a probabilistic error detection model for robotic systems
*** (18) Fault Detection and Diagnosis in Industrial Systems
*** (19; 21) To reject or not to reject: that is the question-an answer in case of neural classifiers
*** (20; 4) Data mining for cyber security
*** (21; 26) A markov chain model of temporal behavior for anomaly detection
*** (22; 3) Overcoming HMM time independence assumption using n-gram based modelling for continuous speech recognition
** COMMENT knowledge-based
*** (5)  Back to the future for consistency-based trajectory tracking
*** (8)  Diagnosing multiple faults
*** (9)  A model-based approach to reactive self-configuring systems
*** (10) Hyde - a general framework for stochastic and hybrid model-based diagnosis
*** (11) Approximation algorithms for model-based diagnosis
*** (12) The Livingstone model of a main propulsion system
*** (13) Lessons learned in the Livingstone 2 on Earth Observing One flight experiment
*** (14) Diagnosis of Autosub 6000 using automatically generated software models
*** (15) Combining particle filters and consistency-based approaches for monitoring and diagnosis of stochastic hybrid systems
*** (16) Diagnosis by a waiter and a Mars explorer
*** (17) Real-time diagnosis and repair of faults of robot control software
* Theory

:PROPERTIES:
:CUSTOM_ID: sec:theory
:END:
######################################################################
# Give a high-level idea & introduce the main theoretical steps
######################################################################

Before we proceed, it is useful to define some vocabulary. The
vocabulary intends to be independent of the terminology of any
specific middleware and instead focus on intuitive words to better
explain the theory. This technique aims to detect faults in software
and so all discussion will implicitly be in this domain unless other
specified. We will consider the whole of a robot's software a system
which are divided into a set of components. Each component does some
job (e.g., a component which takes in laser and map data to determine
the robots location) and the coordination of multiple components is
used to solve some task. Components will be said to subscribe and
publish to one another. If component /A/ subscribes to component /B/,
component /A/ will received all messages published by component /B/. When
a component publishes a message, it is considered an event. These
messages are typed, so, returning to the example of a localizing
component, its message type might be the coordinates required to
described its position in configuration space. Events also have a
type: the metadata required to describe a component, its location in a
robotics system, and the message type. This is necessary because a
component can publish more than one message type and more than one
component can publish the same message type. With this requisite
vocabulary, a formal description of the theory will follow.

#+caption: This example shows a system at three discrete points in time and how data flows through a system. Notice that a message event occurs at a point in time. The goal is to exploit this knowledge to learn a model.
#+label: fig:simple
#+attr_latex: width=0.6\textwidth wrap placement={R}{0.61\textwidth}
[[file:img/simple.pdf]]

The main hypothesis of this approach states that a robotics system is
a set of communicating components which generate temporal
communication patterns when accomplishing tasks. These temporal
communication patterns exhibit structures which depend on the current
state of the robot \cite{Golombek:2010hj}. Because this approach uses
a machine-learned model, it falls completely within the data-driven
approach to fault detection as described in section \ref{lit-review}.
The goal of this technique is to exploit the latent temporal-structure
within the observed communication stream to learn a pattern of
communication which correlates with normal system behavior.

In order to classify the robot as being in a normal or faulty state, a
score is calculated against the learned model. This model represents
the pattern of communication during normal system behavior. More
specifically, the model represents the expectation of the time between
publication events between all event types. Once the score is
calculated, it is compared against a moving threshold to create the
classification of the robot's state at any given time.

######################################################################
# Introduce the example to be used in explaining the idea
######################################################################

To ground the discussion, a simplistic example is shown in figure
\ref{fig:simple}. This graph represents a set of three components and
how messages pass through the system. These components can be seen as
chained together n a linear communication pattern. In this example,
node /A/ publishes message /a/ at timestamp $t$ which passes to node
/B/. Node /B/, after doing some arbitrary computation, publishes a
message /b/ at timestamp $t'$ which is passed to node /C/. So, in this
example, a message event occurs at 100 milliseconds which could be
encoded as =A:a:100ms= and a message event occurs at 150
milliseconds which could be encoded as =B:b:150ms=. It is not
necessary that data flow linearly through a system. In general,
real-life robotics systems exhibit more complicated inter-component
communication patterns. Figure \ref{fig:complex} shows a more complex
system in which components publish multiple messages types and
subscribe to multiple components.

#+caption:    In a real system, data flow will likely be non-linear.
#+label:      fig:ex2
#+attr_latex: width=0.6\textwidth wrap placement={L}{0.55\textwidth}
[[file:img/complex.pdf]]

The rest of this section will use figure \ref{fig:simple} as a simple
example for pedagogic purposes. First, the idea and creation of the
learned model will be explained, followed by the calculation of the
score, and then the calculation of the final classification.

** Learning the model

The learned model exploits the hypothesis that a robot composed of a
set of software components exhibits temporal communication patterns
and that these patterns exhibit different structures depending on the
state of the robot. The goal then becomes to learn the inter-component
communication patterns when the robot is functioning correctly. With
this model, the robot's state can be classified depending on how
closely its current communication patterns adhere to the learned
communication patterns. If the current pattern deviates too far from
the learned pattern, then the robot can be said to be in an anomalous
state. The first step, then, is to create the learned model.

The model is learned by collecting an observation time-series and
learning how components publish with respect to one another. More
formally, let $E$ be the set of encoded time-series of component
communication data which is recorded during normal operation. For each
tuple $(e_i,e_j) \in E \times E$, a probability distribution $P_{ij} =
P(t \vert e_i,e_j)$ is estimated. The distribution $P_{ij}$ represents
the expected timespan of event $e_j$ occurring after event $e_i$. The
event $e_i$ is constrained to be the last seen occurrence of this
event type because the goal is to model temporal correlations between
the current event and the last seen occurrence of a given event type.
Learning the model for the example present back in figure
\ref{fig:simple}, results in a matrix of distributions as shown in the
matrix in \eqref{matrix:ex1}.

\begin{equation}
\label{matrix:ex1}
\begin{bmatrix}
P_{aa} & P_{ab} & P_{ac}\\
P_{ba} & P_{bb} & P_{bc}\\
P_{ca} & P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}

# It should be clear by now that the model does not learn transition
# times between sets of connected components, but instead learns the
# likelihood of the time-span between the publication of message events
# of any two components.

The estimation of $P_{ij}$ makes use of a Kernel Density estimator
which have been initialized with a Gaussian Kernel $K(u) =
\frac{1}{2\pi}e^{-\frac{1}{2}u^2}$ \cite{Golombek:2011ek}. The set of
all learned distributions becomes the model $\mathcal{M} = \{P_{ij}
\vert (e_i,e_j) \in E \times E\}$. $\mathcal{M}$ is now the matrix
shown in \eqref{matrix:ex1}. Figure \ref{fig:learned} makes clear that
a distribution is learned for the Cartesian product of the set of
event types.

#+caption:    A distribution is learned for each pair of event types.
#+label:      fig:learned
#+attr_latex: width=0.5\textwidth wrap placement={R}{0.55\textwidth}
[[file:img/learned.pdf]]

** Calculating the score

During a live run, the score is calculated by comparing the incoming
stream of communication (i.e., message events) to the learned model.
The score is higher the more closely the incoming pattern
matches the learned pattern. Formally, the score at event $e_j$ is
defined as

#+begin_latex
\begin{equation}\label{eq:score}
s_j = \sum_{e_i \in E} w_{ij} \cdot P_{ij}(\Delta{}t_i)
\end{equation}
#+end_latex


\noindent where $E$ is the set of last seen instance of each event
type and $w_{ij}$ is the relative weighting of the probability value.
The weight $w_{ij}$ is a measure of how meaningful the particular
distribution $P_{ij}$ is as an indication to the system's performance.
The weight is defined as

#+begin_latex
\begin{equation}\label{eq:weight}
w_{ij} = 1 - \frac{h_{ij}}{\sum_{e_i \in E} h_{ij}}
\end{equation}
#+end_latex

The weight calculation presented in equation \eqref{eq:weight} makes
use of the entropy of the distribution. This represents how much
information is contained in a particular distribution and its
trustworthiness. Essentially, the lower the entropy, and thus the more
information contained in the distribution, the more willing we are to
trust the correlation between the two event types.

#+begin_latex
\begin{algorithm}
\caption{Calculating the score on the receipt of event $e_j$ with
the set E of last seen instances of all event types.}
\label{alg:score}
\begin{program}
\FUNCT |score|(e_j, E) \BODY
s \gets \sum_{e_i}^E (1 - \frac{h_{ij}}{H_j}) P_{ij}(\Delta(e_i,e_j))
|return | \lVert s \rVert
\WHERE
h_{ij} \equiv \text{ entropy of } P_{ij}
H_j    \equiv \text{ sum entropy of } P_{*j}
\Delta(i,j) \equiv \text{ timespan between events $i$ and $j$}
\END
\end{program}
\end{algorithm}
#+end_latex

** Calculating the threshold

An important aspect of this technique is that as the score changes
over the course of a system run, so does the threshold. What is
considered the threshold for normal behavior is dependent on the
consistency of the communication pattern within the system. The
threshold changes according to formula \eqref{eq:threshold}. The idea
behind this formula is that variance $S_{var}$ of consecutive scores
$S = (s_1, \dotsm, s_{j-1}, s_j)$ is lower when events match the
normal pattern learned in the model $\mathcal{M}$. So, when the
variance is lower, and thus the events better match the learned model,
the threshold is lowered. If the score variance increases, the
threshold increases as well to make the threshold harder to exceed.
This formula is defined formally as

#+begin_latex
\begin{equation}\label{eq:threshold}
s^* = a \cdot s^*_{val} + (1 - a) \cdot s^*_{val} \cdot \frac{S_{var}}{s^*_{var}}
\end{equation}
#+end_latex

where $S_{var}$ is the score variance, $s^*$ is the threshold
variance, and $s^*_{val}$ is a constant minimum threshold which is
determined before runtime.

** Classifying the system

With the score and threshold calculated, classifying the system is
straight forward. As can be seen in \eqref{eq:classification}, the
system is considered abnormal anytime the score of the current event
$e_j$ does not exceed the calculated threshold $s^*$.

#+begin_latex
\begin{equation}\label{eq:classification}
\text{abnormal}(e_j) = \begin{cases}
&\text{true}  : s_j < s^*\\
&\text{false} : else
\end{cases}
\end{equation}
#+end_latex

* Original system
** Implementation

To implement the technique first specified by \cite{Golombek:2010hj}
on CAST, it was necessary to modify the source first implemented by
the original author and create a CAST component to connect to the
modified source. This section will cover the changes made, and the
background knowledge to put it into context, as well as the
description of the CAST component.

######################################################################
# FTS
######################################################################
The original system created at Bielefeld was implemented using the
Filtering, Transformation, and Selection Library (FTS)
\cite{Luetkebohle09-FT}. Using FTS, one decomposes a problem into a
set of nodes which process data in discrete steps. This technique
allows for increased code re-usability due to the fact that nodes can
be connected any number of ways. 

#+caption: The main steps shown in the FTS processing graph representation. Decomposing problems this way allows for high code re-use.
#+attr_latex: width=0.4\textwidth wrap placement={L}{0.41\textwidth}
#+label: fig:fts
[[file:img/fts.pdf]]

In a CAST system, tasks are solved by a set of components grouped into
subarchitectures. Components communicate to one another through a
working memory local to the subarchitecture. Additionally, any
inter-subarchitecture communication also occurs through working
memories. A full explanation of CAST can be found in
\cite{haweswyatt10aei} but is beyond the scope of this dissertation. A
CAST component was created to monitor changes to any working memory
within the system. If a change was detected, metadata about the
message event was created and sent over a network connection to the
fault-detection system. The CAST component could additionally receive
the classification status back from the fault-detect for use by other
components but this was never explored.

** Experimental results

In order to evaluate the system, a series of experiments were create
to test the algorithm. Three different CAST systems were created, each
with properties to push the algorithm (and the changes made to it) in
some way. In each of the following experiments, each component
publishes only a single event type. All experiments were run under the
following conditions

#+ATTR_LaTeX:
+----------------+---------------------+
| Virtualization | Parallels 7.0.15054 |
+----------------+---------------------+
| Guest system   | Ubuntu 10.04 32-bit |
+----------------+---------------------+
| Host system    | Mac OSX    10.7.3   |
+----------------+---------------------+

# *** COMMENT Linear chain
# :PROPERTIES:
# :CUSTOM_ID: sec:3chain
# :END:
# **** Aim & Methodology

# The linear chain was the simplest experiment run on the system. It is
# the exact system presented in figure \ref{fig:simple}. This CAST setup
# was used as a sanity check to ensure that the algorithm could function
# on the simplest case. Failure to work on this case would mean that the
# technique would likely not scale to larger systems.

# The system was run with and without the induction of faults. The goal
# was to see if the system could detect the induced fault without
# flagging other states as fault. Data was collected and averaged over
# 10 runs to prevent skewering by an anomalous run.

# **** Results

*** Parallel chains
:PROPERTIES:
:CUSTOM_ID: sec:4x4
:END:
**** Aim & Methodology

#+begin_src dot :exports none :file "img/4x4.pdf" :cache yes
digraph four_chain {
          rankdir=LR;
          A -> B -> C -> D;
          E -> F -> G -> H;
          I -> J -> K -> L;
          M -> N -> O -> P;
}
#+end_src
#+results[2c5c00e9891f5c001975c3b50767a7f5c481ed3c]:
[[file:img/4x4.pdf]]

#+label: fig:4x4
#+attr_latex: width=0.6\textwidth wrap placement={R}{0.61\textwidth}
[[file:img/4x4.pdf]]

The aim of this experiment was to test the algorithm on a more complex
set of communication patterns. Figure \ref{fig:4x4} shows the layout
of the components within this system. The communication proceeded through
four separate linear chains---each chain completely independent of the
others.

As with the linear system described in section \ref{sec:3chain}, this
system was run with and without the induction of faults. The system
was run 10 times to ensure no anomalous experiment would skew
the final results.

**** Results


#+attr_latex: width=0.6\textwidth wrap placement={R}{0.61\textwidth}
[[file:./img/original_4x4.eps]]

Fault was detected 1.18 seconds after induction.

*** Non-connected components
:PROPERTIES:
:CUSTOM_ID: sec:10x1
:END:
**** Aim & Methodology

#+begin_src dot :exports none :file "img/10x0.pdf" :cache yes
  graph G {
          A;
          B; C; D; E; F; G; H; I; J;
  }
#+end_src
#+results[e12770e1913edc49ff97a14d956f8a319dd77a5a]:

#+attr_latex: width=0.6\textwidth wrap placement={R}{0.61\textwidth}
[[file:img/10x0.pdf]]

This experiment was quite different from the others. It tested a
system in which none of the components communicated with one
another---a non-realistic system---in order to test the algorithm in
key ways. Similarly to the other experiments, this system was run 10
times with and without the induction of faults.

**** Results

#+attr_latex: width=0.8\textwidth
[[file:img/original_10x1.eps]]

Fault was detected 500 milliseconds after induction.

*** COMMENT Dora
** Asymptotic analysis
:PROPERTIES:
:CUSTOM_ID: subsec:orig-asymp
:END:

When evaluating the approach first described in
\cite{Golombek:2010hj}, beyond knowing how it performed
experimentally, it was also desirable to know how the algorithm would
scale with input. This is done by performing asymptotic analysis of
the technique. It is the learned model which is truly core to this
approach and so analysis will focus on the model. There are two
aspects worth analyzing: runtime efficiency of calculating the score
from the model and space efficiency of the model itself.

Space efficiency is concerned with analyzing the amount of memory an
algorithm utilizes as input grows. In the approach described in
section \ref{sec:theory}, we saw that the algorithm learns a
probability distribution for the Cartesian product of the set of event
types. Because this value is constant, we can represent it formally as

\begin{equation}\label{eq:orig_memory}
\text{model}(n) \in  \Theta(n^2)
\end{equation}

This means that as the number of event types $n$ increases, the size
of the model must grow quadratically. During experimentation, it was
observed that with a system of 100 components, memory usage had
exceeded 4 GB.

The runtime efficiency of score calculation was another area of
concern because this algorithm depended directly on the size of the
model. The calculation will be based off of the algorithm
\ref{alg:score} from section \ref{sec:theory}. On analysis, we can see
that there are two aspects to the algorithm: calculating the sum
entropy and then calculating the whole score which can be seen in
equation \eqref{eq:orignal_score}.

#+BEGIN_LATEX
\begin{equation}
\label{eq:orignal_score}
\begin{split}
score(n) &= H_{ij} + \sum_{e_i}^E\\
score(n) &= n + 5n\\
score(n) &= 6n\\
score(n) &\in \Theta(n)
\end{split}
\end{equation}
#+END_LATEX

Since the sum entropy $H_j$ will be the same for all events $e_i \in
E$ on the receipt of event $e_j$, this only needs to be calculated
once. Calculating this value requires a simple summation over the $n$
entries which have information about the event type $j$, hence its
value is $n$. Similarly, the score calculation is a summation over the
$n$ relevant entries in $E$ with the addition of five steps for each
entry, hence $5n$. Performing arithmetic, we can see that while the
model may be $\Theta(n^2)$, the score calculation is only $\Theta(n)$
because it only considers the relevant entries.

* Connection-based model
** Idea & Implementation

The asymptotic analysis from section \ref{subsec:org-asymp} showed
that the space efficiency of the learned model could not scale with
larger systems. The goal for the rest of the project then became to
find a way to reduced the size of this model. The initial idea was to
use the information about the connections between components---all
information which could be gathered /a priori/ on CAST
\cite{Otto:2010uc}. This information could then be exploited to prune
the model and retain only the parts of the model which correlate to
actual paths of communication within the real system.

Using the example presented in figure \ref{fig:simple}, we can see
that informaton flows from node /A/ to node /B/ and from node /B/ to
node /A/. Using the idea of pruning, we could remove from the model
the learned distributions between components /A/ and /C/. Additionally,
since information in this example flows as a directed graph, we can
prune all distributions which correlate to the reverse direction,
e.g., $P_{ab}$. It was decided that the distribution which modeled a
component to itself would be kept because it would be useful to have a
distribution of how often a component fired. All together, the model
$\mathcal{M}$ is reduced to

#+BEGIN_LATEX
\begin{equation}
\label{eq:reduced_model}
\begin{bmatrix}
P_{aa} & \empty & \empty\\
P_{ba} & P_{bb} & \empty\\
\empty& P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}
#+END_LATEX

** Asymptotic analysis

The change in the formulation of the model affects how the size scales
with new event types. Analyzing the space efficiency of this approach,
we can see that in the worst case the system will be fully-connected.
The best case occurs when the system contains no connection between
any components. Formally, the space efficiency of this model is

#+BEGIN_LATEX
\begin{equation}
\begin{split}
\label{eq:reduced_asymp}
model(n) &\in O(n^2)\\
model(n) &\in \Omega(n)
\end{split}
\end{equation}
#+END_LATEX

** Experimental results
* Metronome-based approach
** Idea & Implementation

After the failure of the connection-based approach to reduce the model
and remain performant, a new approach had to be created. What was
created was based on the idea of a metronome, or a heartbeat, and how
it fires at a constant rate. By learning how every other component
fired relative to the metronome, it might be possible to dramatically
reduce the model size while still remaining performant.

To implement, this meant adding an extra component in the CAST system
and pruning all distributions which did not have the metronome =m= in
the $j$ position of a distribution $P_{ij}$. Performing this
optimization example shown in figure \ref{fig:simple} resulted in a
model $\mathcal{M}$ reduced to

#+begin_latex
\begin{equation}
\label{eq:metronome_model}
\begin{bmatrix}
P_{am} & P_{bm} & P_{cm} & P_{mm}
\end{bmatrix}
\end{equation}
#+end_latex

** Asymptotic analysis

Implementing this technique resulted in a far smaller model. Formally,
the space efficiency of this new model became

#+begin_latex
\begin{equation}
\label{eq:metronome_asymp}
model(n) \in \Theta(n + 1)
\end{equation}
#+end_latex

This difference results in a rather dramatic reduction. For example,
on complex CAST system with 100 components, the model size for the
original implementation would be $model(n) \in \Theta(n^2) = 10,000$.
With the metronome approach, the space efficiency for this same system
becomes $model(n) \in \Theta(n + 1) = 101$. The difference in space
efficiency means that the metronome approach could scale more than the
original implementation.

** Experimental results

[[file:img/metronome_4x4.eps]]

[[file:img/metronome_10x1.eps]]

* Project management

Large projects are strenuous. Effective project management then
becomes crucial in ensuring constant progress throughout academic
year.

Git was used rather than Subversion for one key reason: it is easy to
maintain multiple branches of the code and move changes to all of
them. This feature was especially important because it meant that
multiple ideas about the model implementation could be kept in
separate branches. In Subversion, doing the equivalent would have made
it very difficult to make updates to all branches when bugs were found
and fixed.

Because inheriting such a large code-base can be overwhelming, unit
tests were used to create a contract of behavior for the most critical
classes in the system. And by using Jenkins as a continuous
integration server, it was possible to know when any change to the
code caused a test on any branch to fail. Jenkins also published the
results of static analysis run by Maven, the build system used. Static
analysis helped suss out potential bugs and resulted in more robust code.

Perhaps the most important aspect of project management, and
unfortunately discovered only towards the end of the project, was
issue management. It was possible to set project milestones and attach
the issues necessary to complete the milestone. This has the benefit
of putting in concrete terms the steps necessary to reach a goal. So
rather than flailing around to figure out what to do next, there was
always a concrete task that could be done.

** COMMENT Managing tasks & deadlines
*** Github issues
** COMMENT Managing code
*** maven
*** Jenkins
*** git
* Project evaluation

A project

** COMMENT What was good?
*** project planning w.r.t. summer work
** COMMENT What can be learned?
*** sticking with it when intial results are bad
*** setting better goals
* Conclusion

I conclude!

#+begin_comment
#+begin_quote
: In this paper we presented the on-line application of our fault
: detection approach for robotic systems. It is purely data-driven and
: exploits generic information extracted from the systemâ€™s
: inter-component communication. The conducted experiments demonstrated
: that our approach is capable of detecting and tracking various induced
: faults on-line with high probability and acceptable delay. Initially,
: the algorithm suffers form false positives but introducing another
: decision- layer based on a moving average reduces the false positive
: rate. Next steps will involve additional experiments to increase the
: significance of the results and evaluation of novel fault patterns.
: Furthermore, we will extend the detection model with sub-models
: representing different states of the system. This will involve
: different states of normal behaviour as well as states for already
: experienced faults. By this means we will on the one hand improve the
: fault detection and on the other hand enable fault diagnosis for
: experienced faults.
#+end_quote

#+begin_quote
: In this paper we presented a novel method for fault detection in
: robotic systems constructed on the basis of discrete event-based data
: interchange. The introduced self- awareness model is strongly data
: driven and thus (i) can be trained from good examples of normal system
: behavior, (ii) is largely independent from specific scenarios and
: (iii) shows promising results even for transient malfunctions in
: system behavior. The resulting self-awareness model provides a basis
: for a sophisticated autonomic computing architecture in the domain of
: robotic systems, enhancing safety and robustness of robot operation
: and ultimatively increasing the autonomoy of intelligent robot
: systems. Future work will focus on a broader and more realistic daily
: life evaluation as well as an exhaustive complexity and performance
: analysis of the demonstrated approach. Over a long distance our goal
: will be closing the autonomic control loop, effectively allowing
: modification of relevant system properties upon detected anomalies,
: the integration of anomaly detection with behavioral control and the
: further exploration of fault diagnosis models for robotic
: applications.
#+end_quote
#+end_comment

\newpage
\bibliographystyle{plain}
\bibliography{references}



#  LocalWords:  analytical middleware performant metadata runtime
* Graphs                                                           :NOEXPORT:
** Original
*** Linear
#+begin_comment
#+begin_src gnuplot :var data="./data/original-eps-converted-to" :exports none :file "img/reduced_3chain_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src
#+end_comment
*** 4x4

#+begin_src gnuplot :var norm="./data/original_4x4_normal.csv" fault="./data/original_4x4_fault.csv" :exports none :file "img/original_4x4.eps" :cache yes :tangle "data/original_4x4.plt"
  reset
  unset term
  set term postscript eps color solid eps enhanced 20
  set multiplot
  set yrange [0:1]
  set xrange [0:150000]
  set size 0.5, 1.0

  set title 'Normal'
  set origin 0.0, 0.0
  plot norm using 1:2 with dots notitle,\
       norm using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       norm using 1:3 with lines title 'Threshold'
  set title 'Fault'
  set origin 0.5, 0.0
  plot fault using 1:2 with dots notitle,\
       fault using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       fault using 1:3 with lines title 'Threshold'

#+end_src

#+RESULTS[7cd4153226f45a21b472dd5400d2db3560a33f9b]:
[[file:img/original_4x4.eps]]

*** 4x4 normal

#+begin_src gnuplot :var data="./data/original_4x4_normal.csv" :exports none :file "img/original_4x4_normal.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]

  set title 'Normal'
  plot norm using 1:2 with dots notitle,\
       norm using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       norm using 1:3 with lines title 'Threshold'
#+end_src

#+RESULTS[712afaa890dd2a697fe7b6fcc85e9d8f3528686f]:
[[file:img/original_4x4_normal.eps]]

*** 4x4 fault

#+begin_src gnuplot :var data="./data/original_4x4_fault.csv" :exports none :file "img/original_4x4_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  set title "Normal"
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src
#+RESULTS[43081e236f08b1ee98a8982967e878c0ad9f7e27]:
[[file:img/original_4x4_fault.eps]]

* Dot                                                              :NOEXPORT:
** Simple example

#+begin_src dot :exports none :file "img/simple.pdf" :cache yes
  digraph Example1 {
  rankdir=LR;
  subgraph cluster2 {
  label="Event from B";
  A3[label="A"];
  B3[label="B"];
  C3[label="C"];
  A3 -> B3
  [label="a  "];
  B3 -> C3
  [label="b  (150ms)",color="red",style="bold",fontcolor="red"];

  }
  subgraph cluster1 {
  label="Event from A";
  A2[label="A"];
  B2[label="B"];
  C2[label="C"];
  A2 -> B2
  [label="a  (100ms)",color="red",style="bold",fontcolor="red"];
  B2 -> C2 [label="b  "];
  }
  subgraph cluster0 {
  label="No event";
  A1[label="A"];
  B1[label="B"];
  C1[label="C"];
  A1 -> B1 [label="a  "];
  B1 -> C1 [label="b  "];
  }
  }
#+end_src
#+results[fc8897caa2a034eb34782fd9c83ca4451bb52636]:
[[file:img/simple.pdf]]

** Complex example

#+begin_src dot :exports none :file "img/complex.pdf" :cache yes
  digraph real {
  rankdir=LR;
  A -> B [dir="both"];
  A -> C [dir="both"];
  A -> D [dir="both"];
  A -> E [dir="both"];
  A -> F [dir="both"];
  B -> E;
  C -> D;
  D -> E;
  F -> D;
  }
#+end_src

#+results[8033868d33a97b13559b13165338665ffeaaf6df]:
[[file:img/complex.pdf]]

** Learned

#+begin_src dot :exports none :file "img/learned.pdf" :cache yes
  digraph G {
          rankdir=LR;
          A -> A [label="P(AA)"];
          A -> B [label="P(AB)"];
          A -> C [label="P(AC)"];
          B -> A [label="P(BA)"];
          B -> B [label="P(BB)"];
          B -> C [label="P(BC)"];
          C -> A [label="P(CA)"];
          C -> B [label="P(CB)"];
          C -> C [label="P(CC)"];
  }
#+end_src

#+RESULTS[cc2bb741e8fa3d5e6be7049aa932a42ec96640c5]:
[[file:img/learned.pdf]]
** FTS graph

#+begin_src dot :exports none :file "img/fts.pdf" :cache yes
  digraph G {
  CAST;
  CalcScore [label="Calculate Score"];
  ClassifyScore [label="Classify"];
  CAST -> Encode -> CalcScore -> ClassifyScore;
  ClassifyScore -> CAST [style="dotted"];
  }
#+end_src

#+RESULTS[29479010baef6dfc79c12ac1a41b34a2420b283b]:
[[file:img/fts.pdf]]
#  LocalWords:  Virtualization subarchitecture
