#+title:
#+author:  Jeremiah M. Via
#+options: H:4 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+startup: hidestars indent
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LATEX_CLASS: dissertation
#+LATEX_CLASS_OPTIONS: [a4paper,oneside,12pt,onecolumn,final,openany]
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{program}
#+LATEX_HEADER: \usepackage{appendix}
#+LATEX_HEADER: \NumberProgramstrue

######################################################################
# Title page
######################################################################
#+begin_latex
\begin{titlepage}
%% Set the line spacing to 1 for the title page.
\begin{spacing}{1}
\begin{large}
\begin{center}
\mbox{}
\vfill
\begin{sc}
A Data-Driven Self-Awareness Model for Robotics Systems \\
\end{sc}
\vfill
Jeremiah M. Via \\
Supervisor: Nick Hawes \\
\vspace*{4mm}
\includegraphics[width=50mm]{crest.png}\\
Submitted in conformity with the requirements\\
for the degree of Artificial Intelligence \& Computer Science\\
School of Computer Science\\
University of Birmingham\\
\vfill
Copyright {\copyright} 2012 School of Computer Science, University of Birmingham\\
\vspace*{.2in}
\end{center}
\end{large}
\end{spacing}
\end{titlepage}
#+end_latex

######################################################################
# Abstract
######################################################################
#+begin_latex
\thispagestyle{empty}
\newpage
\setcounter{page}{1}
\pagenumbering{roman}

\begin{abstract}
Fault-detection in robotics systems is a difficult task and as systems
are becoming more larger and complex, subtle errors are becoming
harder to diagnose. Traditional fault-detection approaches have relied
on explicit modeling of component behavior, but this technique does
not scale to complex robots operating in dynamic environments. A new
technique which involves making the robot self-aware to the internal
state of its various components is examined. The aim of this project
is to implement and then measure the efficacy of this probabilistic
self-awareness model for the robotics middleware CAST
\cite{haweswyatt10aei}, and if time allows, deal with shortcomings of
the original approach.

\vspace{0.5cm}
\noindent\textit{Keywords}: robotics, fault detection,
machine learning
\end{abstract}
\newpage
#+end_latex

######################################################################
# Acknowledgments
######################################################################
#+begin_latex
  \renewcommand{\abstractname}{}%{Acknowledgments}
  \begin{abstract}
  To Nick, Marc, and Raphael for guiding me on the first project of my
  scientific career.
  \end{abstract}
  \newpage
#+end_latex

######################################################################
# TOC
######################################################################
#+begin_latex
\tableofcontents
\newpage

\setcounter{page}{1}
\pagenumbering{arabic}
#+end_latex

* Motivation                                                          :DRAFT:
:PROPERTIES:
:CUSTOM_ID: motivation
:END:
######################################################################
# Why is it important?
######################################################################
The number and uses of robots is increasing. More and more robots are
becoming part of the daily human experience.
# There are now robots which clean the house, assist in surgery, and
# automate the construction of goods.
There are robots in factories automating difficult, repetitive tasks.
There are an increasing number of domestic robots being used for
assistance in the home and as entertainment. There are even robots
being used as surgical tools. It is important for these robots to
function correctly, and if unable to do so, to degrade gracefully to
minimize harm to themselves and others. To do this, robots need some
way to determine their own operating conditions. Detecting faults
within robotics systems is a hard problem.

The importance of equipping a robot with the ability of self-awareness
increases as humans interact more with robots. One could imagine a
domestic robot which assisted an elderly person or a robot performing
heart surgery. A malfunction in these situations could cause death or
serious injury. One could also imagine robotic arms at a factory
building cars where a malfunction could cause damage to products or
the arm itself resulting in a loss of factory output. These examples
underscore the importance of detecting and handling faults within a
robot system.

######################################################################
# What was the goal of my project?
######################################################################
This project was conducted in collaboration with Raphael Golombek at
Bielefeld University who first implemented this technique on the XCF
middleware \cite{Wrede:2004th}. The main goal for the project was to
take Raphael's software, modify it to work with the robotics
middleware used at the Univeristy of Birmingham---the CoSY
Architecture Schema Toolkit (CAST)---and determine its efficacy. The
impetus behind this was to see if the technique developed by Raphael
could work on another middleware. To determine the efficacy of this
approach, a number of experiments were conducted to test the
algorithm's capabilities. With the time available after the completion
of the main project component, the scalability of the learned model
was examined and alternative formulations proposed.

######################################################################
# Introduce the rest of the dissertation
######################################################################
The rest of the dissertation is organized into six main sections. It
will begin with a literature review which gives a brief overview of
the techniques used by others who have attempted to solve this
problem. The literature review will also show where the technique
presented in this dissertation fits with previous techniques.
Following on from the literature revew, the theory section will cover
the necessary background to understanding the technique presented in
\cite{Golombek:2011ek}. This section should give the reader enough
understanding to implement the technique on any event-based robotics
middleware. After an explanation of the theory, a section will cover
its implementation on CAST. This section will also give some
experimental results as well as asymptotic analysis of the key
portions of the technique. The asymptotic analysis will be an
important motivation for the alternative model formulations presented
in later sections. After covering the original implementation and its
analysis, the next few sections will cover alternative formulations of
the learned model, experimental results, as well as asymptotic
analysis. The differences between these model formulations and the
trade-offs required will be discussed. Finally, the last few sections
will cover project management and present an evaluation of the
project.

* Literature review                                                   :DRAFT:
:PROPERTIES:
:CUSTOM_ID: lit-review
:END:

The problem of closing-the-loop is a problem which has received
considerable attention since at least the 1980s \cite{deKleer:1987vc}.
There are two main approaches to solving this problem: model-based and
data-driven. Model-based approaches have been the domain of more
traditional engineering and are created through exhaustive
specification of component behavior. Data-driven approaches are newer
and leverage machine learning techniques.

Within the domain of model-based approaches, there exists two styles
of techniques. The first is the analytical approach which is used in
problems which are close to the hardware. They depend on a set of
assumptions, rigorous specification, and testing \cite{blanke2006}.
These analytical approaches also require the creation of recovery
routines for all possible faults. Because this approach is so tedious,
work has been made on trying to incorporate the data-driven with
analytical approaches \cite{Luo:2010ud}. The second is the
knowledge-based approach where qualitative models are used. A popular
method has been consistency-based diagnosis \cite{deKleer:1987vc},
which often make use of propositional logic. There has been much work
in this domain on diagnosis engines such as Livingstone
\cite{Kurien:2000ta,Williams:1996wf}, HyDE \cite{Narasimhan:2007ty},
and Lydia \cite{Feldman:2010uy}. The Livingstone has been very
successful, having been used on /Deep Space One/ \cite{Bajwa:2002tm},
/Earth Observing One/ \cite{Hayden:2004vn}, and on the /Autosub 6000/
\cite{Ernits:2010tm}.

Data-driven approaches have used machine learning in an attempt to
bypass the effort involved with model-based approaches. A wide variety
of approaches have been tried, broadly categorized into deterministic
and stochastic \cite{Golombek:2011ek}. Deterministic approaches have
generally attempted to cluster the feature space to separate normal
classes from faulty ones \cite{DeStefano:2000vt,Chandola:2006um}.
Stochastic techniques have assumed that anomalous data points would be
in low probability areas \cite{Casar:2008tp,Ye:2000uu}. The idea then
is to use statistical inference on model which has been fit to
previously gathered data. Generally, the deterministic approaches have
failed to handle the uncertainty present in the complex systems in
robots.The stochastic approaches previously mentioned have depended on
the Markov assumption which does not hold in the complex communication
patterns in a robot \cite{Golombek:2010hj}.

The method used in this project was first described in
\cite{Golombek:2010hj} and \cite{Golombek:2011ek}. It is a purely
data-driven approach which finds structure in the temporal-stream of
communication between software components in a complex robotics
system. The original system was developed to work with the XCF
middleware and for this project has been extended to work the CAST
middleware.

** COMMENT 2010 paper
*** (9) Reliable detection of episodes in event sequences
** COMMENT Who has used the data-driven approach and what did they do?
*** (1) Learning a probabilistic error detection model for robotic systems
*** (18) Fault Detection and Diagnosis in Industrial Systems
*** (19; 21) To reject or not to reject: that is the question-an answer in case of neural classifiers
*** (20; 4) Data mining for cyber security
*** (21; 26) A markov chain model of temporal behavior for anomaly detection
*** (22; 3) Overcoming HMM time independence assumption using n-gram based modelling for continuous speech recognition
** COMMENT knowledge-based
*** (5)  Back to the future for consistency-based trajectory tracking
*** (8)  Diagnosing multiple faults
*** (9)  A model-based approach to reactive self-configuring systems
*** (10) Hyde - a general framework for stochastic and hybrid model-based diagnosis
*** (11) Approximation algorithms for model-based diagnosis
*** (12) The Livingstone model of a main propulsion system
*** (13) Lessons learned in the Livingstone 2 on Earth Observing One flight experiment
*** (14) Diagnosis of Autosub 6000 using automatically generated software models
*** (15) Combining particle filters and consistency-based approaches for monitoring and diagnosis of stochastic hybrid systems
*** (16) Diagnosis by a waiter and a Mars explorer
*** (17) Real-time diagnosis and repair of faults of robot control software
* Theory                                                              :DRAFT:
\label{sec:theory}
:PROPERTIES:
:CUSTOM_ID: sec:theory
:END:
######################################################################
# Give a high-level idea & introduce the main theoretical steps
######################################################################

#+begin_latex
\begin{wrapfigure}{R}{0.61\textwidth}
\centering
\includegraphics[width=0.6\textwidth]{img/simple.pdf}
\caption[A simple system]{This example shows a system at three discrete points in time and how data flows through a system. Notice that a message event occurs at a point in time. The goal is to exploit this knowledge to learn a model.}
\label{fig:simple}
\end{wrapfigure}
#+end_latex

Before we proceed, it is useful to define some vocabulary. The
vocabulary intends to be independent of the terminology of any
specific middleware and instead focus on intuitive words to better
explain the theory. This technique aims to detect faults in software
and so all discussion will implicitly be in this domain unless other
specified. We will consider the whole of a robot's software a system
which are divided into a set of components. Each component does some
job (e.g., a component which takes in laser and map data to determine
the robots location) and the coordination of multiple components is
used to solve some task. Components will be said to subscribe and
publish to one another. If component /A/ subscribes to component /B/,
component /A/ will received all messages published by component /B/. When
a component publishes a message, it is considered an event. These
messages are typed, so, returning to the example of a localizing
component, its message type might be the coordinates required to
described its position in configuration space. Events also have a
type: the metadata required to describe a component, its location in a
robotics system, and the message type. This is necessary because a
component can publish more than one message type and more than one
component can publish the same message type. With this requisite
vocabulary, a formal description of the theory will follow.

The main hypothesis of this approach states that a robotics system is
a set of communicating components which generate temporal
communication patterns when accomplishing tasks. These temporal
communication patterns exhibit structures which depend on the current
state of the robot \cite{Golombek:2010hj}. Because this approach uses
a machine-learned model, it falls completely within the data-driven
approach to fault detection as described in section \ref{lit-review}.
The goal of this technique is to exploit the latent temporal-structure
within the observed communication stream to learn a pattern of
communication which correlates with normal system behavior.

In order to classify the robot as being in a normal or faulty state, a
score is calculated against the learned model. This model represents
the pattern of communication during normal system behavior. More
specifically, the model represents the expectation of the time between
publication events between all event types. Once the score is
calculated, it is compared against a moving threshold to create the
classification of the robot's state at any given time.

######################################################################
# Introduce the example to be used in explaining the idea
######################################################################

To ground the discussion, a simplistic example is shown in figure
\ref{fig:simple}. This graph represents a set of three components and
how messages pass through the system. These components can be seen as
chained together n a linear communication pattern. In this example,
node /A/ publishes message /a/ at timestamp $t$ which passes to node
/B/. Node /B/, after doing some arbitrary computation, publishes a
message /b/ at timestamp $t'$ which is passed to node /C/. So, in this
example, a message event occurs at 100 milliseconds which could be
encoded as =A:a:100ms= and a message event occurs at 150
milliseconds which could be encoded as =B:b:150ms=. It is not
necessary that data flow linearly through a system. In general,
real-life robotics systems exhibit more complicated inter-component
communication patterns. Figure \ref{fig:complex} shows a more complex
system in which components publish multiple messages types and
subscribe to multiple components.

#+begin_latex
\begin{wrapfigure}{L}{0.41\textwidth}
\centering
\includegraphics[width=0.4\textwidth]{img/complex.pdf}
\caption[A complex system]{In a real system, data flow will likely be non-linear.}
\label{fig:complex}
\end{wrapfigure}
#+end_latex

The rest of this section will use figure \ref{fig:simple} as a simple
example for pedagogic purposes. First, the idea and creation of the
learned model will be explained, followed by the calculation of the
score, and then the calculation of the final classification.

** Learning the model                                                :DRAFT:

The learned model exploits the hypothesis that a robot composed of a
set of software components exhibits temporal communication patterns
and that these patterns exhibit different structures depending on the
state of the robot. The goal then becomes to learn the inter-component
communication patterns when the robot is functioning correctly. With
this model, the robot's state can be classified depending on how
closely its current communication patterns adhere to the learned
communication patterns. If the current pattern deviates too far from
the learned pattern, then the robot can be said to be in an anomalous
state. The first step, then, is to create the learned model.

The model is learned by collecting an observation time-series and
learning how components publish with respect to one another. More
formally, let $E$ be the set of encoded time-series of component
communication data which is recorded during normal operation. For each
tuple $(e_i,e_j) \in E \times E$, a probability distribution $P_{ij} =
P(t \vert e_i,e_j)$ is estimated. The distribution $P_{ij}$ represents
the expected timespan of event $e_j$ occurring after event $e_i$. The
event $e_i$ is constrained to be the last seen occurrence of this
event type because the goal is to model temporal correlations between
the current event and the last seen occurrence of a given event type.
Learning the model for the example present back in figure
\ref{fig:simple}, results in a matrix of distributions as shown in the
matrix in \eqref{matrix:ex1}.

\begin{equation}
\label{matrix:ex1}
\begin{bmatrix}
P_{aa} & P_{ab} & P_{ac}\\
P_{ba} & P_{bb} & P_{bc}\\
P_{ca} & P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}

# It should be clear by now that the model does not learn transition
# times between sets of connected components, but instead learns the
# likelihood of the time-span between the publication of message events
# of any two components.

The estimation of $P_{ij}$ makes use of a Kernel Density estimator
which have been initialized with a Gaussian Kernel $K(u) =
\frac{1}{2\pi}e^{-\frac{1}{2}u^2}$ \cite{Golombek:2011ek}. The set of
all learned distributions becomes the model $\mathcal{M} = \{P_{ij}
\vert (e_i,e_j) \in E \times E\}$. $\mathcal{M}$ is now the matrix
shown in \eqref{matrix:ex1}. Figure \ref{fig:learned} makes clear that
a distribution is learned for the Cartesian product of the set of
event types.

#+caption:    A distribution is learned for each pair of event types.
#+label:      fig:learned
#+attr_latex: width=0.5\textwidth wrap placement={R}{0.55\textwidth}
[[file:img/learned.pdf]]

** Calculating the score                                             :DRAFT:

During a live run, the score is calculated by comparing the incoming
stream of communication (i.e., message events) to the learned model.
The score is higher the more closely the incoming pattern
matches the learned pattern. Formally, the score at event $e_j$ is
defined as

#+begin_latex
\begin{equation}\label{eq:score}
s_j = \sum_{e_i \in E} w_{ij} \cdot P_{ij}(\Delta{}t_i)
\end{equation}
#+end_latex


\noindent where $E$ is the set of last seen instance of each event
type and $w_{ij}$ is the relative weighting of the probability value.
The weight $w_{ij}$ is a measure of how meaningful the particular
distribution $P_{ij}$ is as an indication to the system's performance.
The weight is defined as

#+begin_latex
\begin{equation}\label{eq:weight}
w_{ij} = 1 - \frac{h_{ij}}{\sum_{e_i \in E} h_{ij}}
\end{equation}
#+end_latex

The weight calculation presented in equation \eqref{eq:weight} makes
use of the entropy of the distribution. This represents how much
information is contained in a particular distribution and its
trustworthiness. Essentially, the lower the entropy, and thus the more
information contained in the distribution, the more willing we are to
trust the correlation between the two event types.

#+begin_latex
\begin{algorithm}
\caption{Calculating the score on the receipt of event $e_j$ with
the set E of last seen instances of all event types.}
\label{alg:score}
\begin{program}
\FUNCT |score|(e_j, E) \BODY
s \gets \sum_{e_i}^E (1 - \frac{h_{ij}}{H_j}) P_{ij}(\Delta(e_i,e_j))
|return | \lVert s \rVert
\WHERE
h_{ij} \equiv \text{ entropy of } P_{ij}
H_j    \equiv \text{ sum entropy of } P_{*j}
\Delta(i,j) \equiv \text{ timespan between events $i$ and $j$}
\END
\end{program}
\end{algorithm}
#+end_latex

** Calculating the threshold                                         :DRAFT:

An important aspect of this technique is that as the score changes
over the course of a system run, so does the threshold. What is
considered the threshold for normal behavior is dependent on the
consistency of the communication pattern within the system. The
threshold changes according to formula \eqref{eq:threshold}. The idea
behind this formula is that variance $S_{var}$ of consecutive scores
$S = (s_1, \dotsm, s_{j-1}, s_j)$ is lower when events match the
normal pattern learned in the model $\mathcal{M}$. So, when the
variance is lower, and thus the events better match the learned model,
the threshold is lowered. If the score variance increases, the
threshold increases as well to make the threshold harder to exceed.
This formula is defined formally as

#+begin_latex
\begin{equation}\label{eq:threshold}
s^* = a \cdot s^*_{val} + (1 - a) \cdot s^*_{val} \cdot \frac{S_{var}}{s^*_{var}}
\end{equation}
#+end_latex

where $S_{var}$ is the score variance, $s^*$ is the threshold
variance, and $s^*_{val}$ is a constant minimum threshold which is
determined before runtime.

** Classifying the system                                            :DRAFT:

* Original system
** Implementation                                                    :DRAFT:

To implement the technique first specified by \cite{Golombek:2010hj}
on CAST, it was necessary to modify the source first implemented by
the original author and create a CAST component to connect to the
modified source. This section will cover the changes made, and the
background knowledge to put it into context, as well as the
description of the CAST component.

#+begin_latex
\begin{wrapfigure}{R}{0.41\textwidth}
\centering
\includegraphics[width=0.4\textwidth]{img/fts.pdf}
\caption[The FTS graph processor]{The main steps shown in the FTS processing graph representation. Decomposing problems this way allows for high code re-use.}
\label{fig:fts}
\end{wrapfigure}
#+end_latex

######################################################################
# FTS
######################################################################
The original system created at Bielefeld was implemented using the
Filtering, Transformation, and Selection Library (FTS)
\cite{Luetkebohle09-FT}. Using FTS, one decomposes a problem into a
set of nodes which process data in discrete steps. This technique
allows for increased code re-usability due to the fact that nodes can
be connected any number of ways.

In a CAST system, tasks are solved by a set of components grouped into
subarchitectures. Components communicate to one another through a
working memory local to the subarchitecture. Additionally, any
inter-subarchitecture communication also occurs through working
memories. A full explanation of CAST can be found in
\cite{haweswyatt10aei} but is beyond the scope of this dissertation. A
CAST component was created to monitor changes to any working memory
within the system. If a change was detected, metadata about the
message event was created and sent over a network connection to the
fault-detection system. The CAST component could additionally receive
the classification status back from the fault-detector for use by other
components but this was never explored.

** Experimental results

In order to evaluate the system, a series of experiments were created
to test the algorithm. Three different CAST systems were created, each
with properties to push the algorithm (and the changes made to it) in
some way. In each of the following experiments, each component
publishes only a single event type. The following sections will
present the systems tested, the methodology used, and the results of
the experiments.

*** Systems

- Linear chain system :: This is the simple system presented as an
     example back in section \ref{sec:theory} and was used as a sanity
     check when running experiments.
- Parallel chains system :: This system is a more complex version of
     the linear chain system. It is four independent, linear systems.
     The goal with this system was to test how independent chains of
     message events would affect one another.
- Non-connected system :: This system had ten unconnected components.
     It was created to test the ability of the algorithm. It is worth
     nothing that this system does not represent a realistic system
     for solving a task in an event-based architecture.

*** Methodology
\label{subsubsec:experiment_methodology}

The experiments were run in a virtual machine with each system being
run ten times. During the first phase, four-thousand message events
are collected from a normal run of this system. It is this data that
will be used to train the model that is used in the ten runs. In the
second phase, the system is run for another four-thousand message
events with a fault being induced at the two-thousand message event
mark. The performance is analyzed by calculating the delay between
fault induction and fault detection, the sensitivity and
specificity of the fault detector, and the Matthews correlation
coefficient.

The delay being fault induction and fault detection tells us how
quickly the algorithm can detect a fault within the system. The goal
is to detect a fault as quickly as possible. The sensitivity indicates
the likelihood that the fault detector will classify a faulty as being
a faulty state. The specificity indicates the likelihood that a normal
state will be correctly classified. The Matthews correlation
coefficient (MCC) is measure of the agreement between predicted state and
observed state. If the MCC value is +1, it indicates perfect
prediction; if -1, it indicates total disagreement; and if 0, it
indicates random prediction \cite{Baldi:2000wp}.

*** Results

#+caption: Experimental results from the original algorithm.
#+attr_latex: align=|l|r|r|r|r|
#+label: tbl:original
|---------------+-------------+-------------+------+-----------|
|               | Sensitivity | Specificity |  MCC | Delay     |
|---------------+-------------+-------------+------+-----------|
| Linear        |         1.0 |         1.0 |  1.0 | 0.37 sec. |
|---------------+-------------+-------------+------+-----------|
| Parallel      |         1.0 |        0.90 | 0.92 | 0 sec.    |
|---------------+-------------+-------------+------+-----------|
| Non-connected |        0.99 |        0.94 | 0.96 | 0.50 sec. |
|---------------+-------------+-------------+------+-----------|

Table \ref{tbl:original} summarizes the results of the experiments.
The original approach has nearly perfect sensitivity in all
experiments meaning nearly the entirety of the faulty system state.
This might be at the cost of a lower specificity but the specificity
results are also high. The MCC value indicates that the fault detector
prediction almost perfectly matched the ground truth in all
experiments. All faults were detected in less than half of a second.
To be clear, the reason why there is a delay and perfect sensitivity
in the linear experiment is due to the fact that there is a delay in
the calculation of the score. The first system classification after
the fault was induced was faulty, but it took 0.37 seconds for this
classification to occur. The approach was least performant on the
non-connected component system. This was because there was so little
information in the interaction between components for the model to
contain and as a result, when one component was killed the score did
not change much.

** Asymptotic analysis                                               :DRAFT:
\label{subsec:orig-asymp}

When evaluating the approach first described in
\cite{Golombek:2010hj}, beyond knowing how it performed
experimentally, it was also desirable to know how the algorithm would
scale with input. This is done by performing asymptotic analysis of
the technique. It is the learned model which is truly core to this
approach and so analysis will focus on the model. There are two
aspects worth analyzing: runtime efficiency of calculating the score
from the model and space efficiency of the model itself.

Space efficiency is concerned with analyzing the amount of memory an
algorithm utilizes as input grows. In the approach described in
section \ref{sec:theory}, we saw that the algorithm learns a
probability distribution for the Cartesian product of the set of event
types. Because this value is constant, we can represent it formally as

\begin{equation}\label{eq:orig_memory}
\text{model}(n) \in  \Theta(n^2)
\end{equation}

This means that as the number of event types $n$ increases, the size
of the model must grow quadratically. During experimentation, it was
observed that with a system of 100 components, memory usage had
exceeded 4 GB.

The runtime efficiency of score calculation was another area of
concern because this algorithm depended directly on the size of the
model. The calculation will be based off of the algorithm
\ref{alg:score} from section \ref{sec:theory}. On analysis, we can see
that there are two aspects to the algorithm: calculating the sum
entropy and then calculating the whole score which can be seen in
equation \eqref{eq:orignal_score}.

#+begin_latex
\begin{equation}
\label{eq:orignal_score}
\begin{split}
score(n) &= H_{ij} + \sum_{e_i}^E\\
score(n) &= n + 5n\\
score(n) &= 6n\\
score(n) &\in \Theta(n)
\end{split}
\end{equation}
#+end_latex

Since the sum entropy $H_j$ will be the same for all events $e_i \in
E$ on the receipt of event $e_j$, this only needs to be calculated
once. Calculating this value requires a simple summation over the $n$
entries which have information about the event type $j$, hence its
value is $n$. Similarly, the score calculation is a summation over the
$n$ relevant entries in $E$ with the addition of five steps for each
entry, hence $5n$. Performing arithmetic, we can see that while the
model may be $\Theta(n^2)$, the score calculation is only $\Theta(n)$
because it only considers the relevant entries.

* Connection-based model
** Idea & Implementation                                             :DRAFT:

The asymptotic analysis from section \ref{subsec:org-asymp} showed
that the space efficiency of the learned model could not scale with
larger systems. The goal for the rest of the project then became to
find a way to reduced the size of this model. The initial idea was to
use the information about the connections between components---all
information which could be gathered /a priori/ on CAST
\cite{Otto:2010uc}. This information could then be exploited to prune
the model and retain only the parts of the model which correlate to
actual paths of communication within the real system.

Using the example presented in figure \ref{fig:simple}, we can see
that informaton flows from node /A/ to node /B/ and from node /B/ to
node /A/. Using the idea of pruning, we could remove from the model
the learned distributions between components /A/ and /C/. Additionally,
since information in this example flows as a directed graph, we can
prune all distributions which correlate to the reverse direction,
e.g., $P_{ab}$. It was decided that the distribution which modeled a
component to itself would be kept because it would be useful to have a
distribution of how often a component fired. All together, the model
$\mathcal{M}$ is reduced to

#+BEGIN_LATEX
\begin{equation}
\label{eq:reduced_model}
\begin{bmatrix}
P_{aa} & \empty & \empty\\
P_{ba} & P_{bb} & \empty\\
\empty& P_{cb} & P_{cc}
\end{bmatrix}
\end{equation}
#+END_LATEX

** Asymptotic analysis                                               :DRAFT:

The change in the formulation of the model affects how the size scales
with new event types. Analyzing the space efficiency of this approach,
we can see that in the worst case the system will be fully-connected.
The best case occurs when the system contains no connection between
any components. Formally, the space efficiency of this model is

#+BEGIN_LATEX
\begin{equation}
\begin{split}
\label{eq:reduced_asymp}
model(n) &\in O(n^2)\\
model(n) &\in \Omega(n)
\end{split}
\end{equation}
#+END_LATEX

** Experimental results

The same methodology presented in section
\ref{subsubsec:experiment_methodology} was used to evaluate the
connection-based model approach. Table \ref{tbl:reduced} shows the
results from the experiments. What can be seen from the results is
that this approach cannot detect faults. When analyzing the model
against the score calculation this makes sense. The score is
calculated when a message event is received but because the model only
contains distributions for message events originating from the self
component and the message events which the self component subscribes
to, it has a limited capacity to notice that message events are no
longer occurring. Consider the example presented in figure
\ref{fig:simple}: if component /B/ were to die, no more messages
events from components /B/ or /C/ would ever be received. The only
message events which could generate scores would be the messages from
component /A/. If component /A/ is publishing according to the model,
the score would remain high for that message event and this the entire
system. The failure of this approach was the impetus to try the
approach presented in the following section.

#+caption: Experimental results from the connection-based model.
#+attr_latex: align=|l|r|r|r|r|
#+label: tbl:reduced
|---------------+-------------+-------------+-------+----------|
|               | Sensitivity | Specificity |   MCC | Delay    |
|---------------+-------------+-------------+-------+----------|
| Linear        |        0.00 |        0.99 | -0.05 | -        |
|---------------+-------------+-------------+-------+----------|
| Parallel      |        0.00 |        0.95 | -0.18 | -        |
|---------------+-------------+-------------+-------+----------|
| Non-connected |        0.06 |        1.00 |  0.11 | 207 sec. |
|---------------+-------------+-------------+-------+----------|

* Metronome-based approach
** Idea & Implementation                                             :DRAFT:

After the failure of the connection-based approach to reduce the model
and remain performant, a new approach had to be created. What was
created was based on the idea of a metronome, or a heartbeat, and how
it fires at a constant rate. By learning how every other component
fired relative to the metronome, it might be possible to dramatically
reduce the model size while still remaining performant.

To implement, this meant adding an extra component in the CAST system
and pruning all distributions which did not have the metronome =m= in
the $j$ position of a distribution $P_{ij}$. Performing this
optimization example shown in figure \ref{fig:simple} resulted in a
model $\mathcal{M}$ reduced to

#+begin_latex
\begin{equation}
\label{eq:metronome_model}
\begin{bmatrix}
P_{am} & P_{bm} & P_{cm} & P_{mm}
\end{bmatrix}
\end{equation}
#+end_latex

** Asymptotic analysis                                               :DRAFT:

Implementing this technique resulted in a far smaller model. Formally,
the space efficiency of this new model became

#+begin_latex
\begin{equation}
\label{eq:metronome_asymp}
model(n) \in \Theta(n + 1)
\end{equation}
#+end_latex

This difference results in a rather dramatic reduction. For example,
on complex CAST system with 100 components, the model size for the
original implementation would be $model(n) \in \Theta(n^2) = 10,000$.
With the metronome approach, the space efficiency for this same system
becomes $model(n) \in \Theta(n + 1) = 101$. The difference in space
efficiency means that the metronome approach could scale more than the
original implementation.

** Experimental results

#+caption: Experimental results from the metronome model.
#+attr_latex: align=|l|r|r|r|r|
#+label: tbl:metronome
|---------------+-------------+-------------+------+----------|
|               | Sensitivity | Specificity |  MCC | Delay    |
|---------------+-------------+-------------+------+----------|
| Linear        |        0.84 |         1.0 | 0.83 | 6.5 sec. |
|---------------+-------------+-------------+------+----------|
| Parallel      |        0.95 |        0.76 | 0.73 | 1.8 sec. |
|---------------+-------------+-------------+------+----------|
| Non-connected |        0.94 |        0.95 | 0.88 | 9.2 sec. |
|---------------+-------------+-------------+------+----------|

Using the same methodology from section
\ref{subsubsec:experiment_methodology}, experiments were conducted to
analyze the performance of the metronome-based model and table
\ref{tbl:metronome} summarizes the results. It can be seen that in all
cases this system takes longer to detect faults. Given the massive
reduction in model size, the trade-off is likely worth the increased
delay. It is also the case the sensitivity, specificity, and the
Matthews correlation coefficient suffer from this change in the model.
Given that the only change in the system is the formulation of the
model, it is worth investigating if changes to the score and threshold
calculation algorithms could make this approach more performant.

* Project management                                                  :DRAFT:
\label{sec:management}

Large projects are strenuous. Effective project management then
becomes crucial in ensuring constant progress throughout academic
year.

Git was used rather than Subversion for one key reason: it is easy to
maintain multiple branches of the code and move changes to all of
them. This feature was especially important because it meant that
multiple ideas about the model implementation could be kept in
separate branches. In Subversion, doing the equivalent would have made
it very difficult to make updates to all branches when bugs were found
and fixed.

Because inheriting such a large code-base can be overwhelming, unit
tests were used to create a contract of behavior for the most critical
classes in the system. And by using Jenkins as a continuous
integration server, it was possible to know when any change to the
code caused a test on any branch to fail. Jenkins also published the
results of static analysis run by Maven, the build system used. Static
analysis helped suss out potential bugs and resulted in more robust code.

Perhaps the most important aspect of project management, and
unfortunately discovered only towards the end of the project, was
issue management. It was possible to set project milestones and attach
the issues necessary to complete the milestone. This has the benefit
of putting in concrete terms the steps necessary to reach a goal. So
rather than flailing around to figure out what to do next, there was
always a concrete task that could be done.

* Project evaluation                                                  :DRAFT:
\label{sec:evaluation}

This section will focus on evaluating two important aspects of a
project. First, it will evaluate how well the project met its intended
goals. Second, it will evaluate my performance during the course of
this project. For both aspects, things done right will be mentioned as
will areas of improvement for future projects.

The original goals of the project were to modify the code used by
Raphael Golombek, author of the original approach, to work with CAST
and then to determine its efficacy in this system. These goals were
met with enough time to extend the original approach in a meaningful
way.

During the project, I had the habit of wanting to discard an approach
if I did not get the desired results immediately. This fatalistic
approach to science inhibits the discovery of all but the most trivial
new knowledge. Additionally, I had the bad habit of accepting any of
the data that came out of the system without understanding why those
results had occurred. Fortunately, my supervisor Nick helped me start
to break these habits. Because he would ask me to explain why I had
the results I had, I was forced to sit and analyze the system. By
doing this I was able to find and fix a number of bugs in the original
implementation and correct errors in the score calculation which did
not match the papers describing the technique. I am very grateful to
have to learned these lessons sooner in my science career.

I also did a lot of things right during the project. As covered in
section \ref{sec:management}, I did a lot to ease the management and
understanding of such a large, experimental code base. Adding unit
tests to the core classes ensured that my alternative model
formulations could not break the code base in unexpected ways. I also
managed my time well. Knowing that the autumn term would be incredibly
busy, I finished the core of my project over the summer holiday. This
eased the stress I would have otherwise felt and let me focus on that
term's work.

Overall, I did a lot right which made the project easier than it could
have been. I also learned a lot about how to approach the scientific
aspects of the project. These lessons will be useful in graduate
school so I feel lucky for having learned them now.

* Conclusion                                                          :DRAFT:

This dissertation presented the theory behind a new data-driven
technique for detecting faults in event-based robotics middlewares. It
learns a model of the predicted timespan between all pairs of event
types. By exploiting this model, a score for a live system can be
calculated which can be used to classify the robot as being either in
a normal or faulty state. The original implementation was heavily
modified to work with a different middleware than initially designed
and experimental results for this system were presented. Asymptotic
analysis provided motivation for the need to reduced the size of this
learned model. Two alternative model formulations were shown with
experimental results and asymptotic analysis. Experimentation showed
one approach failed to work and so was discarded. Future work on this
topic incorporate multiple models to increase flexibility.
Additionally, models could be learned against known faults which the
robot could utilize to perform specific actions for graceful
degradation. This project took a lot of effort but was well worth it
for the knowledge gained about the topic and on project management.

\newpage
\bibliographystyle{plain}
\bibliography{references}


\appendix\appendixpage\addappheadtotoc
* Experiment graphs
** Original model
*** Linear chain

#+begin_center
[[file:img/original_linear.eps]]
#+end_center

*** Parallel chains

#+begin_center
[[file:img/original_parallel.eps]]
#+end_center

*** Non-connected components

#+begin_center
[[file:img/original_nonconnected.eps]]
#+end_center

** Connection-based model
*** Linear chain

#+begin_center
[[file:img/reduced_linear.eps]]
#+end_center

*** Parallel chains

#+begin_center
[[file:img/reduced_parallel.eps]]
#+end_center

*** Non-connected components

#+begin_center
[[file:img/reduced_nonconnected.eps]]
#+end_center

** Metronome model
*** Linear chain

#+begin_center
[[file:img/metronome_linear.eps]]
#+end_center

*** Parallel chains

#+begin_center
[[file:img/metronome_parallel.eps]]
#+end_center

*** Non-connected components

#+begin_center
[[file:img/metronome_nonconnected.eps]]
#+end_center

* User guide
* Graphs                                                           :NOEXPORT:
** Original
*** Linear

#+begin_src gnuplot :var data="./data/original_3chain_fault.csv" :exports none :file "img/original_linear.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:150000]
  set format x "%3.0f"
  set key below
  fault = 255#255673
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[857ecf0ba9b830ab8504247a9f41f5de22748601]:
[[file:img/original_linear.eps]]

*** Parallel

#+begin_src gnuplot :var data="./data/original_4x4_fault.csv" :exports none :file "img/original_parallel.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  set xrange [0:120]
  set format x "%3.0f"
  set key below
  fault = 51#51878
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[42d138abeea0f0e70706b50f7600dc73fa81be66]:
[[file:img/original_parallel.eps]]

*** Non-connected

#+begin_src gnuplot :var data="./data/original_10x1_fault.csv" :exports none :file "img/original_nonconnected.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 101#101500
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[e7af16506a2820b6664d748d1e20cb0b60cb7e73]:
[[file:img/original_nonconnected.eps]]


*** 4x4 normal

#+begin_src gnuplot :var data="./data/original_4x4_normal.csv" :exports none :file "img/original_4x4_normal.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]

  set title 'Normal'
  plot norm using 1:2 with dots notitle,\
       norm using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       norm using 1:3 with lines title 'Threshold'
#+end_src

#+RESULTS[712afaa890dd2a697fe7b6fcc85e9d8f3528686f]:
[[file:img/original_4x4_normal.eps]]

*** 4x4 fault

#+begin_src gnuplot :var data="./data/original_4x4_fault.csv" :exports none :file "img/original_4x4_fault.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set yrange [0:1]
  set xrange [0:150000]
  set title "Normal"
  plot data using 1:2 with dots notitle,\
       data using 1:2 with lines smooth bezier title 'Score (smoothed)',\
       data using 1:3 with lines title 'Threshold'
#+end_src
#+RESULTS[43081e236f08b1ee98a8982967e878c0ad9f7e27]:
[[file:img/original_4x4_fault.eps]]

** Reduced
*** Linear

#+begin_src gnuplot :var data="./data/reduced_3chain_fault.csv" :exports none :file "img/reduced_linear.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 102
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[bbab97be2f7ea32a6fbe7de3afc7a86be64cac84]:
[[file:img/reduced_linear.eps]]

*** Parallel

#+begin_src gnuplot :var data="./data/reduced_4x4_fault.csv" :exports none :file "img/reduced_parallel.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 64
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[309ac14f19a2ef08284781a59e6f7c9dcf0f54cf]:
[[file:img/reduced_parallel.eps]]

*** Non-connected

#+begin_src gnuplot :var data="./data/reduced_10x1_fault.csv" :exports none :file "img/reduced_nonconnected.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 101
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[4f08cfd84807207fb83a60dd47b6c7cd6a2f36a0]:
[[file:img/reduced_nonconnected.eps]]

** Metronome
*** Linear

#+begin_src gnuplot :var data="./data/metronome_3chain_fault.csv" :exports none :file "img/metronome_linear.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 21
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[221fb4a4721c1723807b78c23d92405a24ef1ea2]:
[[file:img/metronome_linear.eps]]

*** Parallel

#+begin_src gnuplot :var data="./data/metronome_4x4_fault.csv" :exports none :file "img/metronome_parallel.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 18
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[e9b29ec59f1b19cd4b6af0fd6491894c38c92cc6]:
[[file:img/metronome_parallel.eps]]

*** Non-connected

#+begin_src gnuplot :var data="./data/metronome_10x1_fault.csv" :exports none :file "img/metronome_nonconnected.eps" :cache yes
  reset
  set terminal postscript color solid eps enhanced 20
  set parametric
  set yrange [0:1]
  set ylabel 'Score'
  set y2label 'Threshold'
  set xlabel 'Seconds'
  #set xrange [0:200]
  set format x "%3.0f"
  set key below
  fault = 93
  plot data using ($1/1000):($2) with lines  title 'Score',\
       data using ($1/1000):($3) with lines title 'Threshold',\
       fault, t title 'Induced fault'
#+end_src

#+results[c1414d7a1aaba8ba6ea10939296f50127b5e511e]:
[[file:img/metronome_nonconnected.eps]]

* Dot                                                              :NOEXPORT:
** Simple example

#+begin_src dot :exports none :file "img/simple.pdf" :cache yes
  digraph Example1 {
  rankdir=LR;
  subgraph cluster2 {
  label="Event from B";
  A3[label="A"];
  B3[label="B"];
  C3[label="C"];
  A3 -> B3
  [label="a  "];
  B3 -> C3
  [label="b  (150ms)",color="red",style="bold",fontcolor="red"];

  }
  subgraph cluster1 {
  label="Event from A";
  A2[label="A"];
  B2[label="B"];
  C2[label="C"];
  A2 -> B2
  [label="a  (100ms)",color="red",style="bold",fontcolor="red"];
  B2 -> C2 [label="b  "];
  }
  subgraph cluster0 {
  label="No event";
  A1[label="A"];
  B1[label="B"];
  C1[label="C"];
  A1 -> B1 [label="a  "];
  B1 -> C1 [label="b  "];
  }
  }
#+end_src

#+results[28b705f07d1e03abb305d766c3977c98ea4a8c35]:
[[file:img/simple.pdf]]

** Complex example

#+begin_src dot :exports none :file "img/complex.pdf" :cache yes
  digraph real {
  rankdir=LR;
  A -> B [dir="both"];
  A -> C [dir="both"];
  A -> D [dir="both"];
  A -> E [dir="both"];
  A -> F [dir="both"];
  B -> E;
  C -> D;
  D -> E;
  F -> D;
  }
#+end_src

#+results[99f3dcb61f89218e0549f24db3818522474e40b1]:
[[file:img/complex.pdf]]

** Learned

#+begin_src dot :exports none :file "img/learned.pdf" :cache yes
  digraph G {
          rankdir=LR;
          A -> A [label="P(AA)"];
          A -> B [label="P(AB)"];
          A -> C [label="P(AC)"];
          B -> A [label="P(BA)"];
          B -> B [label="P(BB)"];
          B -> C [label="P(BC)"];
          C -> A [label="P(CA)"];
          C -> B [label="P(CB)"];
          C -> C [label="P(CC)"];
  }
#+end_src

#+results[cc2bb741e8fa3d5e6be7049aa932a42ec96640c5]:
[[file:img/learned.pdf]]

** FTS graph

#+begin_src dot :exports none :file "img/fts.pdf" :cache yes
  digraph G {
  CAST;
  CalcScore [label="Calculate Score"];
  ClassifyScore [label="Classify"];
  CAST -> Encode -> CalcScore -> ClassifyScore;
  ClassifyScore -> CAST [style="dotted"];
  }
#+end_src

#+results[5710aa41772addb6164eb3ba5522bf326d7464ce]:
[[file:img/fts.pdf]]

#  LocalWords:  Virtualization subarchitecture timespan
** 10x0 system
#+begin_src dot :exports none :file "img/10x0.pdf" :cache yes
  graph G {
          A;
          B; C; D; E; F; G; H; I; J;
  }
#+end_src

#+results[b8b5f54062138b16fa109b193bbe3784095243ef]:
[[file:img/10x0.pdf]]

** 4x4 system

#+begin_src dot :exports none :file "img/4x4.pdf" :cache yes
digraph four_chain {
          rankdir=LR;
          A -> B -> C -> D;
          E -> F -> G -> H;
          I -> J -> K -> L;
          M -> N -> O -> P;
}
#+end_src

#+results[102e7bb2e2a92621d961c874d899b77a207c512b]:
[[file:img/4x4.pdf]]

** Dora

#+begin_src dot :exports none :file "img/dora.pdf" :cache yes
digraph four_chain {
          rankdir=LR;
          A -> B -> C -> D;
          E -> F -> G -> H;
          I -> J -> K -> L;
          M -> N -> O -> P;
}
#+end_src

#+results[9256b7f8aa8b1fdab31ab3f2e0eec2527c138cb7]:
[[file:img/dora.pdf]]

#  LocalWords:  analytical middleware performant metadata runtime
